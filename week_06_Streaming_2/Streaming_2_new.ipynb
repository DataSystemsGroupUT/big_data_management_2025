{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data exists\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from delta import *\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Create SparkSession with Delta Lake support\n",
    "builder = SparkSession.builder.appName(\"SensorDataWindow\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "# Define schema for the sensor data\n",
    "schema = StructType([\n",
    "    StructField(\"time\", IntegerType(), True),  # time is in epoch seconds\n",
    "    StructField(\"action\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Path to the folder containing the sensor data CSV files\n",
    "sensor_data_path = \"input/sensor-data/\"\n",
    "\n",
    "# Read the sensor data stream\n",
    "sensor_data_df = (spark.readStream\n",
    "                  .schema(schema)  # Define the schema explicitly\n",
    "                  .json(sensor_data_path)  # Read JSON files as a stream\n",
    ")\n",
    "\n",
    "# Create a timestamp column from the epoch time (if it's not already a timestamp)\n",
    "sensor_data_df = sensor_data_df.withColumn(\"timestamp\", F.from_unixtime(F.col(\"time\")))\n",
    "\n",
    "# Define the checkpoint and output path for Delta\n",
    "checkpoint_path = \"streaming/sensor_data_tumbling/_checkpoint\"\n",
    "output_path = \"delta/sensor_data_tumbling\"\n",
    "\n",
    "# Ensure the output path exists\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# Apply tumbling window (every 5 minutes) and count the number of actions in each window\n",
    "sensor_data_window_query = (sensor_data_df\n",
    "                            .groupBy(F.window(\"timestamp\", \"5 minutes\"), \"action\")  # Tumbling window on timestamp (every 5 minutes)\n",
    "                            .count()  # Count the number of actions in each window\n",
    "                            .withColumnRenamed(\"count\", \"action_count\")\n",
    "                            .writeStream\n",
    "                            .outputMode(\"complete\")  # Use complete mode to update the complete table in each trigger\n",
    "                            .format(\"delta\")\n",
    "                            .queryName(\"sensor_data_tumbling_query\")\n",
    "                            .trigger(processingTime=\"5 seconds\")  # Trigger every 5 seconds\n",
    "                            .option(\"checkpointLocation\", checkpoint_path)  # Checkpoint location\n",
    "                            .start(output_path)  # Delta table output path\n",
    ")\n",
    "\n",
    "# Function to create table if not exists\n",
    "def create_table_if_exists(output_path, table_name):\n",
    "    data_exists = False\n",
    "    for _i in range(60):  # Retry for 60 seconds\n",
    "        try:\n",
    "            time.sleep(1)\n",
    "            files = os.listdir(output_path)\n",
    "            for _f in files:\n",
    "                if \".parquet\" in _f:\n",
    "                    if len(os.listdir(f\"{output_path}/_delta_log\")) > 0:\n",
    "                        print(\"data exists\")\n",
    "                        data_exists = True\n",
    "                        break\n",
    "            if data_exists:\n",
    "                spark.sql(f\"CREATE TABLE IF NOT EXISTS {table_name} USING DELTA LOCATION '{output_path}'\")\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(e)  # Uncomment if you want to see exceptions\n",
    "            pass\n",
    "\n",
    "# Ensure table creation\n",
    "table_name = \"sensor_data_tumbling\"\n",
    "create_table_if_exists(output_path, table_name)\n",
    "\n",
    "# Wait for the streaming query to terminate\n",
    "sensor_data_window_query.awaitTermination(timeout = 20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark configuration note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>window</th>\n",
       "      <th>action</th>\n",
       "      <th>action_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>(2016-07-26 02:45:00, 2016-07-26 02:50:00)</td>\n",
       "      <td>Open</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>(2016-07-26 02:50:00, 2016-07-26 02:55:00)</td>\n",
       "      <td>Open</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1104</th>\n",
       "      <td>(2016-07-26 02:50:00, 2016-07-26 02:55:00)</td>\n",
       "      <td>Close</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>(2016-07-26 02:55:00, 2016-07-26 03:00:00)</td>\n",
       "      <td>Open</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>859</th>\n",
       "      <td>(2016-07-26 02:55:00, 2016-07-26 03:00:00)</td>\n",
       "      <td>Close</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593</th>\n",
       "      <td>(2016-07-28 06:25:00, 2016-07-28 06:30:00)</td>\n",
       "      <td>Close</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1044</th>\n",
       "      <td>(2016-07-28 06:30:00, 2016-07-28 06:35:00)</td>\n",
       "      <td>Close</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>831</th>\n",
       "      <td>(2016-07-28 06:35:00, 2016-07-28 06:40:00)</td>\n",
       "      <td>Close</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(2016-07-28 06:40:00, 2016-07-28 06:45:00)</td>\n",
       "      <td>Close</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>(2016-07-28 06:45:00, 2016-07-28 06:50:00)</td>\n",
       "      <td>Close</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1226 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          window action  action_count\n",
       "56    (2016-07-26 02:45:00, 2016-07-26 02:50:00)   Open            32\n",
       "377   (2016-07-26 02:50:00, 2016-07-26 02:55:00)   Open            66\n",
       "1104  (2016-07-26 02:50:00, 2016-07-26 02:55:00)  Close             5\n",
       "641   (2016-07-26 02:55:00, 2016-07-26 03:00:00)   Open            81\n",
       "859   (2016-07-26 02:55:00, 2016-07-26 03:00:00)  Close             6\n",
       "...                                          ...    ...           ...\n",
       "593   (2016-07-28 06:25:00, 2016-07-28 06:30:00)  Close            14\n",
       "1044  (2016-07-28 06:30:00, 2016-07-28 06:35:00)  Close            15\n",
       "831   (2016-07-28 06:35:00, 2016-07-28 06:40:00)  Close            12\n",
       "9     (2016-07-28 06:40:00, 2016-07-28 06:45:00)  Close             3\n",
       "729   (2016-07-28 06:45:00, 2016-07-28 06:50:00)  Close             3\n",
       "\n",
       "[1226 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.format(\"delta\").load(\"delta/sensor_data_tumbling\")\n",
    "df.toPandas().sort_values(by=['window'], ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'delta/sensor_data_sliding/_delta_log'\n",
      "[Errno 2] No such file or directory: 'delta/sensor_data_sliding/_delta_log'\n",
      "[Errno 2] No such file or directory: 'delta/sensor_data_sliding/_delta_log'\n",
      "[Errno 2] No such file or directory: 'delta/sensor_data_sliding/_delta_log'\n",
      "[Errno 2] No such file or directory: 'delta/sensor_data_sliding/_delta_log'\n",
      "[Errno 2] No such file or directory: 'delta/sensor_data_sliding/_delta_log'\n",
      "data exists\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from delta import *\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Create SparkSession with Delta Lake support\n",
    "builder = SparkSession.builder.appName(\"SensorDataWindow\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "# Define schema for the sensor data\n",
    "schema = StructType([\n",
    "    StructField(\"time\", IntegerType(), True),  # time is in epoch seconds\n",
    "    StructField(\"action\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Path to the folder containing the sensor data CSV files\n",
    "sensor_data_path = \"input/sensor-data/\"\n",
    "\n",
    "# Read the sensor data stream\n",
    "sensor_data_df = (spark.readStream\n",
    "                  .schema(schema)  # Define the schema explicitly\n",
    "                  .json(sensor_data_path)  # Read JSON files as a stream\n",
    ")\n",
    "\n",
    "# Create a timestamp column from the epoch time (if it's not already a timestamp)\n",
    "sensor_data_df = sensor_data_df.withColumn(\"timestamp\", F.from_unixtime(F.col(\"time\")))\n",
    "\n",
    "# Define the checkpoint and output path for Delta\n",
    "checkpoint_path = \"streaming/sensor_data_sliding/_checkpoint\"\n",
    "output_path = \"delta/sensor_data_sliding\"\n",
    "\n",
    "# Ensure the output path exists\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# Apply tumbling window (every 5 minutes) and count the number of actions in each window\n",
    "sensor_data_window_query = (sensor_data_df\n",
    "                            .groupBy(F.window(\"timestamp\", \"5 minutes\", \"1 minute\"), \"action\")  # Tumbling window on timestamp (every 5 minutes)\n",
    "                            .count()  # Count the number of actions in each window\n",
    "                            .withColumnRenamed(\"count\", \"action_count\")\n",
    "                            .writeStream\n",
    "                            .outputMode(\"complete\")  # Use complete mode to update the complete table in each trigger\n",
    "                            .format(\"delta\")\n",
    "                            .queryName(\"sensor_data_tumbling_query\")\n",
    "                            .trigger(processingTime=\"5 seconds\")  # Trigger every 5 seconds\n",
    "                            .option(\"checkpointLocation\", checkpoint_path)  # Checkpoint location\n",
    "                            .start(output_path)  # Delta table output path\n",
    ")\n",
    "\n",
    "# Function to create table if not exists\n",
    "def create_table_if_exists(output_path, table_name):\n",
    "    data_exists = False\n",
    "    for _i in range(60):  # Retry for 60 seconds\n",
    "        try:\n",
    "            time.sleep(1)\n",
    "            files = os.listdir(output_path)\n",
    "            for _f in files:\n",
    "                if \".parquet\" in _f:\n",
    "                    if len(os.listdir(f\"{output_path}/_delta_log\")) > 0:\n",
    "                        print(\"data exists\")\n",
    "                        data_exists = True\n",
    "                        break\n",
    "            if data_exists:\n",
    "                spark.sql(f\"CREATE TABLE IF NOT EXISTS {table_name} USING DELTA LOCATION '{output_path}'\")\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(e)  # Uncomment if you want to see exceptions\n",
    "            pass\n",
    "\n",
    "# Ensure table creation\n",
    "table_name = \"sensor_data_sliding\"\n",
    "create_table_if_exists(output_path, table_name)\n",
    "\n",
    "# Wait for the streaming query to terminate\n",
    "sensor_data_window_query.awaitTermination(timeout = 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>window</th>\n",
       "      <th>action</th>\n",
       "      <th>action_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3352</th>\n",
       "      <td>(2016-07-26 02:41:00, 2016-07-26 02:46:00)</td>\n",
       "      <td>Open</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2324</th>\n",
       "      <td>(2016-07-26 02:42:00, 2016-07-26 02:47:00)</td>\n",
       "      <td>Open</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4249</th>\n",
       "      <td>(2016-07-26 02:43:00, 2016-07-26 02:48:00)</td>\n",
       "      <td>Open</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2726</th>\n",
       "      <td>(2016-07-26 02:44:00, 2016-07-26 02:49:00)</td>\n",
       "      <td>Open</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>(2016-07-26 02:45:00, 2016-07-26 02:50:00)</td>\n",
       "      <td>Open</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>966</th>\n",
       "      <td>(2016-07-28 06:44:00, 2016-07-28 06:49:00)</td>\n",
       "      <td>Close</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2283</th>\n",
       "      <td>(2016-07-28 06:45:00, 2016-07-28 06:50:00)</td>\n",
       "      <td>Close</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4967</th>\n",
       "      <td>(2016-07-28 06:46:00, 2016-07-28 06:51:00)</td>\n",
       "      <td>Close</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>657</th>\n",
       "      <td>(2016-07-28 06:47:00, 2016-07-28 06:52:00)</td>\n",
       "      <td>Close</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2924</th>\n",
       "      <td>(2016-07-28 06:48:00, 2016-07-28 06:53:00)</td>\n",
       "      <td>Close</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6136 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          window action  action_count\n",
       "3352  (2016-07-26 02:41:00, 2016-07-26 02:46:00)   Open             2\n",
       "2324  (2016-07-26 02:42:00, 2016-07-26 02:47:00)   Open             4\n",
       "4249  (2016-07-26 02:43:00, 2016-07-26 02:48:00)   Open            15\n",
       "2726  (2016-07-26 02:44:00, 2016-07-26 02:49:00)   Open            22\n",
       "217   (2016-07-26 02:45:00, 2016-07-26 02:50:00)   Open            32\n",
       "...                                          ...    ...           ...\n",
       "966   (2016-07-28 06:44:00, 2016-07-28 06:49:00)  Close             3\n",
       "2283  (2016-07-28 06:45:00, 2016-07-28 06:50:00)  Close             3\n",
       "4967  (2016-07-28 06:46:00, 2016-07-28 06:51:00)  Close             2\n",
       "657   (2016-07-28 06:47:00, 2016-07-28 06:52:00)  Close             2\n",
       "2924  (2016-07-28 06:48:00, 2016-07-28 06:53:00)  Close             1\n",
       "\n",
       "[6136 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.format(\"delta\").load(\"delta/sensor_data_sliding\")\n",
    "df.toPandas().sort_values(by=['window'], ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8307cf20-40d9-4eef-9d0b-af50f5901b50",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Structured Streaming\n",
    "  \n",
    "  \n",
    "* Kafka \n",
    "* Aggregations\n",
    "* Time windows\n",
    "* Watermarking\n",
    "* Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "475b45b8-9b3d-412e-9da5-0291489a5214",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Time windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_cleaned_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d432333a-1335-4fc4-8b5d-1e79f16ebbf9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# commonly, you might not want an aggregation of a stream's whole history.\n",
    "# for this purpose, let's use window from functions - NB this is \"time windows\" not \"SQL-like (row) window function\"\n",
    "\n",
    "checkpoint_path = \"streaming/orders_most_products_tumb/_checkpoint\" \n",
    "table_name = \"orders_most_products_tumb\"\n",
    "output_path = f\"spark-warehouse/{table_name}\"\n",
    "\n",
    "orders_most_products_tumb_query = (orders_cleaned_df\n",
    "  .select(\"user_id\",F.size(\"product_ids\").alias(\"count_of_products\"), \"order_timestamp\")\n",
    "  .groupBy(\"user_id\", F.window(\"order_timestamp\", \"5 minute\")) # Aggregate by user, every 5 minute block. This is a \"tumbling window\"\n",
    "  .sum(\"count_of_products\")\n",
    "  .withColumnRenamed(\"sum(count_of_products)\", \"product_count\")\n",
    "  .writeStream\n",
    "  .outputMode(\"complete\") # we overwrite the complete table with every trigger\n",
    "  .format(\"delta\")\n",
    "  .queryName(\"orders_most_products_tumb_query\")\n",
    "  .trigger(processingTime=\"5 second\")\n",
    "  .option(\"checkpointLocation\", checkpoint_path)\n",
    "  .start(output_path) \n",
    ")\n",
    "\n",
    "create_table_if_exists(output_path,table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_table_if_exists(output_path,table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.table(table_name).orderBy(F.desc(\"window.end\"),F.desc(\"product_count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e86c785c-51ac-4780-be1e-184e01fee543",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# if we want to keep always the latest time window then we can use sliding windows\n",
    "\n",
    "checkpoint_path = \"streaming/orders_most_products_slide/_checkpoint\" \n",
    "table_name = \"orders_most_products_slide\"\n",
    "output_path = f\"spark-warehouse/{table_name}\"\n",
    "\n",
    "orders_most_products_slide_query = (orders_cleaned_df\n",
    "  .select(\"user_id\",F.size(\"product_ids\").alias(\"count_of_products\"), \"order_timestamp\")\n",
    "  .groupBy(\"user_id\", F.window(\"order_timestamp\", \"5 minute\", \"1 minute\")) # Aggregate by user, every 5 minute block sliding by 1 minute.\n",
    "  .sum(\"count_of_products\")\n",
    "  .withColumnRenamed(\"sum(count_of_products)\", \"product_count\")\n",
    "  .writeStream\n",
    "  .outputMode(\"complete\") # we overwrite the complete table with every trigger\n",
    "  .format(\"delta\")\n",
    "  .queryName(\"orders_most_products_slide_query\")\n",
    "  .trigger(processingTime=\"5 second\")\n",
    "  .option(\"checkpointLocation\", checkpoint_path)\n",
    "  .start(output_path) \n",
    ")\n",
    "\n",
    "create_table_if_exists(output_path,table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.table(table_name).orderBy(F.desc(\"window.end\"),F.desc(\"product_count\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "051067d9-2417-4562-bf14-5f55b0347200",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Non-Kafka part starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acac1e02-d555-4fdc-aa57-0eb83b64c55d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# let's try with a different, simpler dataset\n",
    "\n",
    "input_path = \"sensor-data\"\n",
    "\n",
    "json_schema = \"time timestamp, action string\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5f36054-4522-42b8-97aa-7fabeede5da0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's create a dataframe and apply some transformations and aggregation\n",
    "\n",
    "input_df = (spark\n",
    "  .readStream                                 \n",
    "  .schema(json_schema)                       \n",
    "  .option(\"maxFilesPerTrigger\", 1)            \n",
    "  .json(input_path)                           \n",
    ")\n",
    "\n",
    "counts_df = (input_df\n",
    "  .groupBy(F.col(\"action\"),                     # Aggregate by action\n",
    "           F.window(F.col(\"time\"), \"1 hour\"))     # and by a 1 hour window\n",
    "  .count()                                    # Count the actions\n",
    "  .select(F.col(\"window.start\").alias(\"start\"), \n",
    "          F.col(\"count\"),                       \n",
    "          F.col(\"action\"))                      \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aea650a6-ed4e-44a2-8eb7-65d73326a046",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"streaming/counts/_checkpoint\" \n",
    "table_name = \"counts\"\n",
    "output_path = f\"spark-warehouse/{table_name}\"\n",
    "\n",
    "counts_query = (counts_df\n",
    "  .writeStream\n",
    "  .outputMode(\"complete\")\n",
    "  .format(\"delta\")\n",
    "  .queryName(\"counts_query\")\n",
    "  .trigger(processingTime=\"10 second\")\n",
    "  .option(\"checkpointLocation\", checkpoint_path)\n",
    "  .start(output_path) \n",
    ")\n",
    "\n",
    "create_table_if_exists(output_path,table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.table(table_name).orderBy(F.col(\"start\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e200d92c-a0ce-4b7c-be79-3658e2e56dd5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Watermarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d968b6b-01a9-480a-8d63-b3e5c0ca159a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# in actual use cases, the queries above would keep running for a very long time and the amount of windows would grow indefinitely\n",
    "# keeping track of all the states puts pressure on memory\n",
    "# also it may often be irrelevant if delayed data updates our figures\n",
    "\n",
    "watermarked_stream = \"watermarked_stream\"\n",
    "\n",
    "watermarked_df = (input_df\n",
    "  .withWatermark(\"time\", \"2 hours\")             # Specify a 2-hour watermark\n",
    "  .groupBy(F.col(\"action\"),                       # Aggregate by action...\n",
    "           F.window(F.col(\"time\"), \"1 hour\"))       # ...then by a 1 hour window\n",
    "  .count()                                      # For each aggregate, produce a count\n",
    "  .select(F.col(\"window.start\").alias(\"start\"),   # Elevate field to column\n",
    "          F.col(\"count\"),                         # Include count\n",
    "          F.col(\"action\"))                        # Include action\n",
    ")\n",
    "display(watermarked_df, streamName = watermarked_stream) # Start the stream and display it\n",
    "\n",
    "# important note: watermarking guarantees that any event within the window gets in. It does not guarantee leaving anything out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in actual use cases, the queries above would keep running for a very long time and the amount of windows would grow indefinitely\n",
    "# keeping track of all the states puts pressure on memory\n",
    "# also it may often be irrelevant if delayed data updates our figures\n",
    "\n",
    "checkpoint_path = \"streaming/orders_most_products_slide_wm/_checkpoint\" \n",
    "table_name = \"orders_most_products_slide_wm\"\n",
    "output_path = f\"spark-warehouse/{table_name}\"\n",
    "\n",
    "orders_most_products_slide_wm_query = (orders_cleaned_df\n",
    "  .withWatermark(\"order_timestamp\", \"20 minute\")             # Specify a 20-minute watermark\n",
    "  .select(\"user_id\",F.size(\"product_ids\").alias(\"count_of_products\"), \"order_timestamp\")\n",
    "  .groupBy(\"user_id\", F.window(\"order_timestamp\", \"5 minute\", \"1 minute\")) # Aggregate by user, every 5 minute block sliding by 1 minute.\n",
    "  .sum(\"count_of_products\")\n",
    "  .withColumnRenamed(\"sum(count_of_products)\", \"product_count\")\n",
    "  .writeStream\n",
    "  .outputMode(\"complete\") # we overwrite the complete table with every trigger\n",
    "  .format(\"delta\")\n",
    "  .queryName(\"orders_most_products_slide_wm_query\")\n",
    "  .trigger(processingTime=\"5 second\")\n",
    "  .option(\"checkpointLocation\", checkpoint_path)\n",
    "  .start(output_path) \n",
    ")\n",
    "\n",
    "create_table_if_exists(output_path,table_name)\n",
    "\n",
    "# important note: watermarking guarantees that any event within the window gets in. It does not guarantee leaving anything out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.table(table_name).orderBy(F.desc(\"window.end\"),F.desc(\"product_count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf4aa920-978f-4e07-ab18-efca64d10f51",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's import another dataset. Let's say we are interested in hourly monitoring of incoming traffic to our website\n",
    "\n",
    "schema = \"device STRING, ecommerce STRUCT<purchase_revenue_in_usd: DOUBLE, total_item_quantity: BIGINT, unique_items: BIGINT>, event_name STRING, event_previous_timestamp BIGINT, event_timestamp BIGINT, geo STRUCT<city: STRING, state: STRING>, items ARRAY<STRUCT<coupon: STRING, item_id: STRING, item_name: STRING, item_revenue_in_usd: DOUBLE, price_in_usd: DOUBLE, quantity: BIGINT>>, traffic_source STRING, user_first_touch_timestamp BIGINT, user_id STRING\"\n",
    "\n",
    "hourlyEventsPath = \"events20200703\"\n",
    "\n",
    "website_df = (spark.readStream\n",
    "  .schema(schema)\n",
    "  .option(\"maxFilesPerTrigger\", 1)\n",
    "  .json(hourlyEventsPath)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81d948ed-3a26-45e7-8e92-836cc07ae9c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# this dataframe does not have a proper timestamp column. So we need to create one and use it for watermarking\n",
    "\n",
    "events_df = (website_df\n",
    "             .withColumn(\"createdAt\", (F.col(\"event_timestamp\") / 1e6).cast(\"timestamp\"))\n",
    "             .withWatermark(\"createdAt\", \"2 hours\")\n",
    ")             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb7b62a6-8a6d-41bc-a226-5c5d7cf40943",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# now we can do an aggregation\n",
    "\n",
    "traffic_df = (events_df\n",
    "             .groupBy(\"traffic_source\"\n",
    "                      , F.window(F.col(\"createdAt\"), \"1 hour\"))\n",
    "             .agg(F.approx_count_distinct(\"user_id\").alias(\"active_users\"))\n",
    "             .select(F.col(\"traffic_source\")\n",
    "                     , F.col(\"active_users\")\n",
    "                     , F.hour(F.col(\"window.start\")).alias(\"hour\"))\n",
    "             .sort(\"hour\")\n",
    ")\n",
    "\n",
    "\n",
    "checkpoint_path = \"streaming/traffic/_checkpoint\" \n",
    "table_name = \"traffic\"\n",
    "output_path = f\"spark-warehouse/{table_name}\"\n",
    "\n",
    "traffic_query = (traffic_df\n",
    "  .writeStream\n",
    "  .outputMode(\"complete\") # we overwrite the complete table with every trigger\n",
    "  .format(\"delta\")\n",
    "  .queryName(\"traffic_query\")\n",
    "  .trigger(processingTime=\"10 second\")\n",
    "  .option(\"checkpointLocation\", checkpoint_path)\n",
    "  .start(output_path) \n",
    ")\n",
    "\n",
    "create_table_if_exists(output_path,table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.table(table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "307411ab-59c4-4026-b6d8-65b745ca1d28",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Joining streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d2d54db-d035-4163-a9af-679884a3edb1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# let's load in users dataset\n",
    "\n",
    "users_df = spark.read.parquet(\"users.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "600645dc-8cb3-47ba-8d28-bc6330cefdb0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# join works same way as with regular dataframes.\n",
    "# note: this is streaming<->static join\n",
    "\n",
    "joined_df = (events_df\n",
    "            .join(users_df.drop(\"user_first_touch_timestamp\"), \"user_id\")\n",
    "            )\n",
    "\n",
    "checkpoint_path = \"streaming/join_static/_checkpoint\" \n",
    "table_name = \"join_static\"\n",
    "output_path = f\"spark-warehouse/{table_name}\"\n",
    "\n",
    "join_static_query = (joined_df\n",
    "  .writeStream\n",
    "  .outputMode(\"append\")\n",
    "  .format(\"delta\")\n",
    "  .queryName(\"join_static_query\")\n",
    "  .trigger(processingTime=\"10 second\")\n",
    "  .option(\"checkpointLocation\", checkpoint_path)\n",
    "  .start(output_path) \n",
    ")\n",
    "\n",
    "create_table_if_exists(output_path,table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.table(table_name).limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da8f8dbd-c601-4ed6-b96a-0af6abbabf36",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# let's read in users dataframe as a stream\n",
    "\n",
    "# since we have created the dataframe from this data, we can cheat on getting the schema. Possible in development/debugging, not possible or recommended in production\n",
    "users_schema = users_df.schema\n",
    "\n",
    "users_stream_df = (spark\n",
    "                   .readStream\n",
    "                   .format(\"parquet\")\n",
    "                   .schema(users_schema)\n",
    "                   .option(\"maxFilesPerTrigger\", 1)\n",
    "                   .parquet(\"users.parquet\")\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eab5049a-272a-468b-a80a-0b25830b93fe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# let's do a stream to stream join\n",
    "\n",
    "joined_streams_df = (events_df\n",
    "            .join(users_stream_df.drop(\"user_first_touch_timestamp\"), \"user_id\")\n",
    "            )\n",
    "\n",
    "checkpoint_path = \"streaming/join_stream/_checkpoint\" \n",
    "table_name = \"join_stream\"\n",
    "output_path = f\"spark-warehouse/{table_name}\"\n",
    "\n",
    "join_stream_query = (joined_df\n",
    "  .writeStream\n",
    "  .outputMode(\"append\")\n",
    "  .format(\"delta\")\n",
    "  .queryName(\"join_stream_query\")\n",
    "  .trigger(processingTime=\"10 second\")\n",
    "  .option(\"checkpointLocation\", checkpoint_path)\n",
    "  .start(output_path) \n",
    ")\n",
    "\n",
    "create_table_if_exists(output_path,table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.table(table_name).limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6ddcecc-b7c5-427e-bbd7-fea0f4886160",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for stream in spark.streams.active:\n",
    "  stream.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2ffb003-abb3-4689-9ad0-8564ac25e1cb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Further reading\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.window.html  \n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.withWatermark.html  \n",
    "https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html  \n",
    "https://docs.databricks.com/spark/latest/structured-streaming/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b127445-2b2f-49fb-84fe-fc9063f2450f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Task 1\n",
    "\n",
    "Create a streaming dataframe from the data in the following path:  \n",
    "`flights200701stream`\n",
    "\n",
    "The schema should contain\n",
    "* DepartureAt (timestamp)\n",
    "* UniqueCarrier (string)\n",
    "\n",
    "Process only 1 file per trigger.  \n",
    "\n",
    "Aggregate the data by count, using non-overlapping 30 minute windows.  \n",
    "Ignore any data that is older than 6 hours.\n",
    "\n",
    "The output should have 3 columns: startTime (window start time), UniqueCarrier, count.  \n",
    "\n",
    "Save to a delta table, firing the trigger every 5 seconds.\n",
    "\n",
    "Display the table, the output should be sorted ascending by startTime.\n",
    "\n",
    "Once the stream has produced some output, call the stream shutdown function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4204d127-eb1f-4c1f-be71-dd4c638935f2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Your answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "Join the Kafka streaming orders dataframe to the `product.csv` dataset.  \n",
    "Note that Spark assumes that any streaming dataframes refer to a directory, not a specific file.\n",
    "\n",
    "Aggregate the data by sum(price) (`total_price`) and a 2-minute tumbling window.  \n",
    "Add a 10 minute watermark, and store the data in a delta table.\n",
    "\n",
    "Create a view on top of the delta table with the following columns:\n",
    "* product_id\n",
    "* product_name\n",
    "* n_minus_2_window_total_price\n",
    "* n_minus_1_window_total_price\n",
    "* current_window_total_price\n",
    "\n",
    "The total_price columns need to be pivoted based on only the 3 most recent windows. \n",
    "The actual \n",
    "Order the dataset by descending `current_total_price`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Practice session - Structured Streaming continued",
   "notebookOrigID": 3621743426785043,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
