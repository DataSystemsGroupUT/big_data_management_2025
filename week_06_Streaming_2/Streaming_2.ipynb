{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structured Streaming\n",
    "  \n",
    "  \n",
    "* Kafka - Using a different docker container\n",
    "* Time windows\n",
    "* Aggregations\n",
    "* Watermarking\n",
    "* Joins\n",
    "\n",
    "## Time Window  \n",
    "\n",
    "Time windows allow us to perform aggregations on streaming data over specific time intervals. They help in analyzing time-based patterns efficiently.  \n",
    "- Tumbling Time Window\n",
    "- Sliding Time Window\n",
    "\n",
    "### Tumbling Time Window  \n",
    "- Fixed-size, non-overlapping windows (e.g., every 5 minutes).  \n",
    "- Each event belongs to exactly one window.  \n",
    "- Useful for analyzing distinct time intervals without overlap.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data exists\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from delta import *\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Create SparkSession with Delta Lake support\n",
    "builder = SparkSession.builder.appName(\"SensorDataWindow\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "# Define schema for the sensor data\n",
    "schema = StructType([\n",
    "    StructField(\"time\", IntegerType(), True),  # time is in epoch seconds\n",
    "    StructField(\"action\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Path to the folder containing the sensor data CSV files\n",
    "sensor_data_path = \"input/sensor-data/\"\n",
    "\n",
    "# Read the sensor data stream\n",
    "sensor_data_df = (spark.readStream\n",
    "                  .schema(schema)  # Define the schema explicitly\n",
    "                  .json(sensor_data_path)  # Read JSON files as a stream\n",
    ")\n",
    "\n",
    "# Create a timestamp column from the epoch time (if it's not already a timestamp)\n",
    "sensor_data_df = sensor_data_df.withColumn(\"timestamp\", F.from_unixtime(F.col(\"time\")))\n",
    "\n",
    "# Define the checkpoint and output path for Delta\n",
    "checkpoint_path = \"streaming/sensor_data_tumbling/_checkpoint\"\n",
    "output_path = \"delta/sensor_data_tumbling\"\n",
    "\n",
    "# Ensure the output path exists\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# Apply tumbling window (every 5 minutes) and count the number of actions in each window\n",
    "sensor_data_window_query = (sensor_data_df\n",
    "                            .groupBy(F.window(\"timestamp\", \"5 minutes\"), \"action\")  # Tumbling window on timestamp (every 5 minutes)\n",
    "                            .count()  # Count the number of actions in each window\n",
    "                            .withColumnRenamed(\"count\", \"action_count\")\n",
    "                            .writeStream\n",
    "                            .outputMode(\"complete\")  # Use complete mode to update the complete table in each trigger\n",
    "                            .format(\"delta\")\n",
    "                            .queryName(\"sensor_data_tumbling_query\")\n",
    "                            .trigger(processingTime=\"5 seconds\")  # Trigger every 5 seconds\n",
    "                            .option(\"checkpointLocation\", checkpoint_path)  # Checkpoint location\n",
    "                            .start(output_path)  # Delta table output path\n",
    ")\n",
    "\n",
    "# Function to create table if not exists\n",
    "def create_table_if_exists(output_path, table_name):\n",
    "    data_exists = False\n",
    "    for _i in range(60):  # Retry for 60 seconds\n",
    "        try:\n",
    "            time.sleep(1)\n",
    "            files = os.listdir(output_path)\n",
    "            for _f in files:\n",
    "                if \".parquet\" in _f:\n",
    "                    if len(os.listdir(f\"{output_path}/_delta_log\")) > 0:\n",
    "                        print(\"data exists\")\n",
    "                        data_exists = True\n",
    "                        break\n",
    "            if data_exists:\n",
    "                spark.sql(f\"CREATE TABLE IF NOT EXISTS {table_name} USING DELTA LOCATION '{output_path}'\")\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(e)  # Uncomment if you want to see exceptions\n",
    "            pass\n",
    "\n",
    "# Ensure table creation\n",
    "table_name = \"sensor_data_tumbling\"\n",
    "create_table_if_exists(output_path, table_name)\n",
    "\n",
    "# Wait for the streaming query to terminate\n",
    "sensor_data_window_query.awaitTermination(timeout = 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>window</th>\n",
       "      <th>action</th>\n",
       "      <th>action_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>(2016-07-26 02:45:00, 2016-07-26 02:50:00)</td>\n",
       "      <td>Open</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>(2016-07-26 02:50:00, 2016-07-26 02:55:00)</td>\n",
       "      <td>Open</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1104</th>\n",
       "      <td>(2016-07-26 02:50:00, 2016-07-26 02:55:00)</td>\n",
       "      <td>Close</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>(2016-07-26 02:55:00, 2016-07-26 03:00:00)</td>\n",
       "      <td>Open</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>859</th>\n",
       "      <td>(2016-07-26 02:55:00, 2016-07-26 03:00:00)</td>\n",
       "      <td>Close</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593</th>\n",
       "      <td>(2016-07-28 06:25:00, 2016-07-28 06:30:00)</td>\n",
       "      <td>Close</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1044</th>\n",
       "      <td>(2016-07-28 06:30:00, 2016-07-28 06:35:00)</td>\n",
       "      <td>Close</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>831</th>\n",
       "      <td>(2016-07-28 06:35:00, 2016-07-28 06:40:00)</td>\n",
       "      <td>Close</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(2016-07-28 06:40:00, 2016-07-28 06:45:00)</td>\n",
       "      <td>Close</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>(2016-07-28 06:45:00, 2016-07-28 06:50:00)</td>\n",
       "      <td>Close</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1226 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          window action  action_count\n",
       "56    (2016-07-26 02:45:00, 2016-07-26 02:50:00)   Open            32\n",
       "377   (2016-07-26 02:50:00, 2016-07-26 02:55:00)   Open            66\n",
       "1104  (2016-07-26 02:50:00, 2016-07-26 02:55:00)  Close             5\n",
       "641   (2016-07-26 02:55:00, 2016-07-26 03:00:00)   Open            81\n",
       "859   (2016-07-26 02:55:00, 2016-07-26 03:00:00)  Close             6\n",
       "...                                          ...    ...           ...\n",
       "593   (2016-07-28 06:25:00, 2016-07-28 06:30:00)  Close            14\n",
       "1044  (2016-07-28 06:30:00, 2016-07-28 06:35:00)  Close            15\n",
       "831   (2016-07-28 06:35:00, 2016-07-28 06:40:00)  Close            12\n",
       "9     (2016-07-28 06:40:00, 2016-07-28 06:45:00)  Close             3\n",
       "729   (2016-07-28 06:45:00, 2016-07-28 06:50:00)  Close             3\n",
       "\n",
       "[1226 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.format(\"delta\").load(\"delta/sensor_data_tumbling\")\n",
    "df.toPandas().sort_values(by=['window'], ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Sliding Time Window  \n",
    "- Fixed-size, overlapping windows (e.g., a 5-minute window sliding every 1 minute).  \n",
    "- Each event can belong to multiple windows.  \n",
    "- Useful for more granular trend analysis over time.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'delta/sensor_data_sliding/_delta_log'\n",
      "[Errno 2] No such file or directory: 'delta/sensor_data_sliding/_delta_log'\n",
      "[Errno 2] No such file or directory: 'delta/sensor_data_sliding/_delta_log'\n",
      "[Errno 2] No such file or directory: 'delta/sensor_data_sliding/_delta_log'\n",
      "[Errno 2] No such file or directory: 'delta/sensor_data_sliding/_delta_log'\n",
      "[Errno 2] No such file or directory: 'delta/sensor_data_sliding/_delta_log'\n",
      "data exists\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Read the sensor data stream\n",
    "sensor_data_df = (spark.readStream\n",
    "                  .schema(schema)  # Define the schema explicitly\n",
    "                  .json(sensor_data_path)  # Read JSON files as a stream\n",
    ")\n",
    "\n",
    "# Create a timestamp column from the epoch time (if it's not already a timestamp)\n",
    "sensor_data_df = sensor_data_df.withColumn(\"timestamp\", F.from_unixtime(F.col(\"time\")))\n",
    "\n",
    "# Define the checkpoint and output path for Delta\n",
    "checkpoint_path = \"streaming/sensor_data_sliding/_checkpoint\"\n",
    "output_path = \"delta/sensor_data_sliding\"\n",
    "\n",
    "# Ensure the output path exists\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# Apply tumbling window (every 5 minutes) and count the number of actions in each window\n",
    "sensor_data_window_query = (sensor_data_df\n",
    "                            .groupBy(F.window(\"timestamp\", \"5 minutes\", \"1 minute\"), \"action\")  # Tumbling window on timestamp (every 5 minutes)\n",
    "                            .count()  # Count the number of actions in each window\n",
    "                            .withColumnRenamed(\"count\", \"action_count\")\n",
    "                            .writeStream\n",
    "                            .outputMode(\"complete\")  # Use complete mode to update the complete table in each trigger\n",
    "                            .format(\"delta\")\n",
    "                            .queryName(\"sensor_data_tumbling_query\")\n",
    "                            .trigger(processingTime=\"5 seconds\")  # Trigger every 5 seconds\n",
    "                            .option(\"checkpointLocation\", checkpoint_path)  # Checkpoint location\n",
    "                            .start(output_path)  # Delta table output path\n",
    ")\n",
    "\n",
    "# Ensure table creation\n",
    "table_name = \"sensor_data_sliding\"\n",
    "create_table_if_exists(output_path, table_name)\n",
    "\n",
    "# Wait for the streaming query to terminate\n",
    "sensor_data_window_query.awaitTermination(timeout = 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>window</th>\n",
       "      <th>action</th>\n",
       "      <th>action_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3352</th>\n",
       "      <td>(2016-07-26 02:41:00, 2016-07-26 02:46:00)</td>\n",
       "      <td>Open</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2324</th>\n",
       "      <td>(2016-07-26 02:42:00, 2016-07-26 02:47:00)</td>\n",
       "      <td>Open</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4249</th>\n",
       "      <td>(2016-07-26 02:43:00, 2016-07-26 02:48:00)</td>\n",
       "      <td>Open</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2726</th>\n",
       "      <td>(2016-07-26 02:44:00, 2016-07-26 02:49:00)</td>\n",
       "      <td>Open</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>(2016-07-26 02:45:00, 2016-07-26 02:50:00)</td>\n",
       "      <td>Open</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>966</th>\n",
       "      <td>(2016-07-28 06:44:00, 2016-07-28 06:49:00)</td>\n",
       "      <td>Close</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2283</th>\n",
       "      <td>(2016-07-28 06:45:00, 2016-07-28 06:50:00)</td>\n",
       "      <td>Close</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4967</th>\n",
       "      <td>(2016-07-28 06:46:00, 2016-07-28 06:51:00)</td>\n",
       "      <td>Close</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>657</th>\n",
       "      <td>(2016-07-28 06:47:00, 2016-07-28 06:52:00)</td>\n",
       "      <td>Close</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2924</th>\n",
       "      <td>(2016-07-28 06:48:00, 2016-07-28 06:53:00)</td>\n",
       "      <td>Close</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6136 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          window action  action_count\n",
       "3352  (2016-07-26 02:41:00, 2016-07-26 02:46:00)   Open             2\n",
       "2324  (2016-07-26 02:42:00, 2016-07-26 02:47:00)   Open             4\n",
       "4249  (2016-07-26 02:43:00, 2016-07-26 02:48:00)   Open            15\n",
       "2726  (2016-07-26 02:44:00, 2016-07-26 02:49:00)   Open            22\n",
       "217   (2016-07-26 02:45:00, 2016-07-26 02:50:00)   Open            32\n",
       "...                                          ...    ...           ...\n",
       "966   (2016-07-28 06:44:00, 2016-07-28 06:49:00)  Close             3\n",
       "2283  (2016-07-28 06:45:00, 2016-07-28 06:50:00)  Close             3\n",
       "4967  (2016-07-28 06:46:00, 2016-07-28 06:51:00)  Close             2\n",
       "657   (2016-07-28 06:47:00, 2016-07-28 06:52:00)  Close             2\n",
       "2924  (2016-07-28 06:48:00, 2016-07-28 06:53:00)  Close             1\n",
       "\n",
       "[6136 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.format(\"delta\").load(\"delta/sensor_data_sliding\")\n",
    "df.toPandas().sort_values(by=['window'], ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acac1e02-d555-4fdc-aa57-0eb83b64c55d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Json schema instead of structype\n",
    "json_schema = \"time timestamp, action string\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5f36054-4522-42b8-97aa-7fabeede5da0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's create a dataframe and apply some transformations and aggregation\n",
    "\n",
    "input_df = (spark\n",
    "  .readStream                                 \n",
    "  .schema(json_schema)                       \n",
    "  .option(\"maxFilesPerTrigger\", 1)            \n",
    "  .json(input_path)                           \n",
    ")\n",
    "\n",
    "counts_df = (input_df\n",
    "  .groupBy(F.col(\"action\"),                     # Aggregate by action\n",
    "           F.window(F.col(\"time\"), \"1 hour\"))     # and by a 1 hour window\n",
    "  .count()                                    # Count the actions\n",
    "  .select(F.col(\"window.start\").alias(\"start\"), \n",
    "          F.col(\"count\"),                       \n",
    "          F.col(\"action\"))                      \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aea650a6-ed4e-44a2-8eb7-65d73326a046",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"streaming/counts/_checkpoint\" \n",
    "table_name = \"counts\"\n",
    "output_path = f\"spark-warehouse/{table_name}\"\n",
    "\n",
    "counts_query = (counts_df\n",
    "  .writeStream\n",
    "  .outputMode(\"complete\")\n",
    "  .format(\"delta\")\n",
    "  .queryName(\"counts_query\")\n",
    "  .trigger(processingTime=\"10 second\")\n",
    "  .option(\"checkpointLocation\", checkpoint_path)\n",
    "  .start(output_path) \n",
    ")\n",
    "\n",
    "create_table_if_exists(output_path,table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.table(table_name).orderBy(F.col(\"start\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e200d92c-a0ce-4b7c-be79-3658e2e56dd5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Watermarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d968b6b-01a9-480a-8d63-b3e5c0ca159a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# in actual use cases, the queries above would keep running for a very long time and the amount of windows would grow indefinitely\n",
    "# keeping track of all the states puts pressure on memory\n",
    "# also it may often be irrelevant if delayed data updates our figures\n",
    "\n",
    "watermarked_stream = \"watermarked_stream\"\n",
    "\n",
    "watermarked_df = (input_df\n",
    "  .withWatermark(\"time\", \"2 hours\")             # Specify a 2-hour watermark\n",
    "  .groupBy(F.col(\"action\"),                       # Aggregate by action...\n",
    "           F.window(F.col(\"time\"), \"1 hour\"))       # ...then by a 1 hour window\n",
    "  .count()                                      # For each aggregate, produce a count\n",
    "  .select(F.col(\"window.start\").alias(\"start\"),   # Elevate field to column\n",
    "          F.col(\"count\"),                         # Include count\n",
    "          F.col(\"action\"))                        # Include action\n",
    ")\n",
    "display(watermarked_df, streamName = watermarked_stream) # Start the stream and display it\n",
    "\n",
    "# important note: watermarking guarantees that any event within the window gets in. It does not guarantee leaving anything out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf4aa920-978f-4e07-ab18-efca64d10f51",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's import another dataset. Let's say we are interested in hourly monitoring of incoming traffic to our website\n",
    "\n",
    "schema = \"device STRING, ecommerce STRUCT<purchase_revenue_in_usd: DOUBLE, total_item_quantity: BIGINT, unique_items: BIGINT>, event_name STRING, event_previous_timestamp BIGINT, event_timestamp BIGINT, geo STRUCT<city: STRING, state: STRING>, items ARRAY<STRUCT<coupon: STRING, item_id: STRING, item_name: STRING, item_revenue_in_usd: DOUBLE, price_in_usd: DOUBLE, quantity: BIGINT>>, traffic_source STRING, user_first_touch_timestamp BIGINT, user_id STRING\"\n",
    "\n",
    "hourlyEventsPath = \"input/events20200703\"\n",
    "\n",
    "website_df = (spark.readStream\n",
    "  .schema(schema)\n",
    "  .option(\"maxFilesPerTrigger\", 1)\n",
    "  .json(hourlyEventsPath)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81d948ed-3a26-45e7-8e92-836cc07ae9c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# this dataframe does not have a proper timestamp column. So we need to create one and use it for watermarking\n",
    "\n",
    "events_df = (website_df\n",
    "             .withColumn(\"createdAt\", (F.col(\"event_timestamp\") / 1e6).cast(\"timestamp\"))\n",
    "             .withWatermark(\"createdAt\", \"2 hours\")\n",
    ")             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb7b62a6-8a6d-41bc-a226-5c5d7cf40943",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data exists\n"
     ]
    }
   ],
   "source": [
    "# now we can do an aggregation\n",
    "\n",
    "traffic_df = (events_df\n",
    "             .groupBy(\"traffic_source\"\n",
    "                      , F.window(F.col(\"createdAt\"), \"1 hour\"))\n",
    "             .agg(F.approx_count_distinct(\"user_id\").alias(\"active_users\"))\n",
    "             .select(F.col(\"traffic_source\")\n",
    "                     , F.col(\"active_users\")\n",
    "                     , F.hour(F.col(\"window.start\")).alias(\"hour\"))\n",
    "             .sort(\"hour\")\n",
    ")\n",
    "\n",
    "\n",
    "checkpoint_path = \"streaming/traffic/_checkpoint\" \n",
    "table_name = \"traffic\"\n",
    "output_path = f\"spark-warehouse/{table_name}\"\n",
    "\n",
    "traffic_query = (traffic_df\n",
    "  .writeStream\n",
    "  .outputMode(\"complete\") # we overwrite the complete table with every trigger\n",
    "  .format(\"delta\")\n",
    "  .queryName(\"traffic_query\")\n",
    "  .trigger(processingTime=\"10 second\")\n",
    "  .option(\"checkpointLocation\", checkpoint_path)\n",
    "  .start(output_path) \n",
    ")\n",
    "\n",
    "create_table_if_exists(output_path,table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "307411ab-59c4-4026-b6d8-65b745ca1d28",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Joining streams\n",
    "In streaming data processing, we often need to enrich real-time events with additional data. This can be done through streaming-static joins (joining a real-time stream with a static dataset) or streaming-streaming joins (joining two real-time streams).\n",
    "\n",
    "In this section, we perform a streaming-static join, where a continuous event stream is joined with a static user dataset to add user information to incoming events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d2d54db-d035-4163-a9af-679884a3edb1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# let's load in users dataset\n",
    "\n",
    "users_df = spark.read.parquet(\"input/users.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "600645dc-8cb3-47ba-8d28-bc6330cefdb0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# join works same way as with regular dataframes.\n",
    "# note: this is streaming<->static join\n",
    "\n",
    "joined_df = (events_df\n",
    "            .join(users_df.drop(\"user_first_touch_timestamp\"), \"user_id\")\n",
    "            )\n",
    "\n",
    "checkpoint_path = \"streaming/join_static/_checkpoint\" \n",
    "table_name = \"join_static\"\n",
    "output_path = f\"spark-warehouse/{table_name}\"\n",
    "\n",
    "join_static_query = (joined_df\n",
    "  .writeStream\n",
    "  .outputMode(\"append\")\n",
    "  .format(\"delta\")\n",
    "  .queryName(\"join_static_query\")\n",
    "  .trigger(processingTime=\"10 second\")\n",
    "  .option(\"checkpointLocation\", checkpoint_path)\n",
    "  .start(output_path) \n",
    ")\n",
    "\n",
    "create_table_if_exists(output_path,table_name)\n",
    "join_static_query.awaitTermination(timeout = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>device</th>\n",
       "      <th>ecommerce</th>\n",
       "      <th>event_name</th>\n",
       "      <th>event_previous_timestamp</th>\n",
       "      <th>event_timestamp</th>\n",
       "      <th>geo</th>\n",
       "      <th>items</th>\n",
       "      <th>traffic_source</th>\n",
       "      <th>user_first_touch_timestamp</th>\n",
       "      <th>hour</th>\n",
       "      <th>createdAt</th>\n",
       "      <th>email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UA000000105784116</td>\n",
       "      <td>macOS</td>\n",
       "      <td>(None, None, None)</td>\n",
       "      <td>mattresses</td>\n",
       "      <td>1.593356e+15</td>\n",
       "      <td>1593816605189501</td>\n",
       "      <td>(Schertz, TX)</td>\n",
       "      <td>[]</td>\n",
       "      <td>email</td>\n",
       "      <td>1593353545834835</td>\n",
       "      <td>22</td>\n",
       "      <td>2020-07-03 22:50:05.189501</td>\n",
       "      <td>williamsmichael43@gmail.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UA000000105784116</td>\n",
       "      <td>macOS</td>\n",
       "      <td>(None, None, None)</td>\n",
       "      <td>add_item</td>\n",
       "      <td>1.593817e+15</td>\n",
       "      <td>1593816671760502</td>\n",
       "      <td>(Schertz, TX)</td>\n",
       "      <td>[(NEWBED10, M_PREM_T, Premium Twin Mattress, 9...</td>\n",
       "      <td>email</td>\n",
       "      <td>1593353545834835</td>\n",
       "      <td>22</td>\n",
       "      <td>2020-07-03 22:51:11.760502</td>\n",
       "      <td>williamsmichael43@gmail.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UA000000105826647</td>\n",
       "      <td>Windows</td>\n",
       "      <td>(None, None, None)</td>\n",
       "      <td>mattresses</td>\n",
       "      <td>1.593360e+15</td>\n",
       "      <td>1593816307373967</td>\n",
       "      <td>(San Diego, CA)</td>\n",
       "      <td>[]</td>\n",
       "      <td>email</td>\n",
       "      <td>1593360175850522</td>\n",
       "      <td>22</td>\n",
       "      <td>2020-07-03 22:45:07.373967</td>\n",
       "      <td>lindsey32@sanchez.info</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UA000000105826647</td>\n",
       "      <td>Windows</td>\n",
       "      <td>(None, None, None)</td>\n",
       "      <td>add_item</td>\n",
       "      <td>1.593816e+15</td>\n",
       "      <td>1593817141171761</td>\n",
       "      <td>(San Diego, CA)</td>\n",
       "      <td>[(NEWBED10, M_STAN_T, Standard Twin Mattress, ...</td>\n",
       "      <td>email</td>\n",
       "      <td>1593360175850522</td>\n",
       "      <td>22</td>\n",
       "      <td>2020-07-03 22:59:01.171761</td>\n",
       "      <td>lindsey32@sanchez.info</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UA000000105826647</td>\n",
       "      <td>Windows</td>\n",
       "      <td>(None, None, None)</td>\n",
       "      <td>cart</td>\n",
       "      <td>1.593817e+15</td>\n",
       "      <td>1593817173411564</td>\n",
       "      <td>(San Diego, CA)</td>\n",
       "      <td>[(NEWBED10, M_STAN_T, Standard Twin Mattress, ...</td>\n",
       "      <td>email</td>\n",
       "      <td>1593360175850522</td>\n",
       "      <td>22</td>\n",
       "      <td>2020-07-03 22:59:33.411564</td>\n",
       "      <td>lindsey32@sanchez.info</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607162</th>\n",
       "      <td>UA000000107203756</td>\n",
       "      <td>Android</td>\n",
       "      <td>(None, None, None)</td>\n",
       "      <td>mattresses</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1593813352318609</td>\n",
       "      <td>(Kirby, TX)</td>\n",
       "      <td>[]</td>\n",
       "      <td>google</td>\n",
       "      <td>1593813352318609</td>\n",
       "      <td>21</td>\n",
       "      <td>2020-07-03 21:55:52.318609</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607163</th>\n",
       "      <td>UA000000107204051</td>\n",
       "      <td>Linux</td>\n",
       "      <td>(None, None, None)</td>\n",
       "      <td>mattresses</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1593813431096000</td>\n",
       "      <td>(Austin, TX)</td>\n",
       "      <td>[]</td>\n",
       "      <td>google</td>\n",
       "      <td>1593813431096000</td>\n",
       "      <td>21</td>\n",
       "      <td>2020-07-03 21:57:11.096000</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607164</th>\n",
       "      <td>UA000000107204375</td>\n",
       "      <td>Android</td>\n",
       "      <td>(None, None, None)</td>\n",
       "      <td>main</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1593813539955034</td>\n",
       "      <td>(Leawood, KS)</td>\n",
       "      <td>[]</td>\n",
       "      <td>instagram</td>\n",
       "      <td>1593813539955034</td>\n",
       "      <td>21</td>\n",
       "      <td>2020-07-03 21:58:59.955034</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607165</th>\n",
       "      <td>UA000000107204421</td>\n",
       "      <td>Android</td>\n",
       "      <td>(None, None, None)</td>\n",
       "      <td>main</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1593813553558908</td>\n",
       "      <td>(Corsicana, TX)</td>\n",
       "      <td>[]</td>\n",
       "      <td>google</td>\n",
       "      <td>1593813553558908</td>\n",
       "      <td>21</td>\n",
       "      <td>2020-07-03 21:59:13.558908</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607166</th>\n",
       "      <td>UA000000107204430</td>\n",
       "      <td>Android</td>\n",
       "      <td>(None, None, None)</td>\n",
       "      <td>main</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1593813556338287</td>\n",
       "      <td>(Taylorville, IL)</td>\n",
       "      <td>[]</td>\n",
       "      <td>google</td>\n",
       "      <td>1593813556338287</td>\n",
       "      <td>21</td>\n",
       "      <td>2020-07-03 21:59:16.338287</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>607167 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  user_id   device           ecommerce  event_name  \\\n",
       "0       UA000000105784116    macOS  (None, None, None)  mattresses   \n",
       "1       UA000000105784116    macOS  (None, None, None)    add_item   \n",
       "2       UA000000105826647  Windows  (None, None, None)  mattresses   \n",
       "3       UA000000105826647  Windows  (None, None, None)    add_item   \n",
       "4       UA000000105826647  Windows  (None, None, None)        cart   \n",
       "...                   ...      ...                 ...         ...   \n",
       "607162  UA000000107203756  Android  (None, None, None)  mattresses   \n",
       "607163  UA000000107204051    Linux  (None, None, None)  mattresses   \n",
       "607164  UA000000107204375  Android  (None, None, None)        main   \n",
       "607165  UA000000107204421  Android  (None, None, None)        main   \n",
       "607166  UA000000107204430  Android  (None, None, None)        main   \n",
       "\n",
       "        event_previous_timestamp   event_timestamp                geo  \\\n",
       "0                   1.593356e+15  1593816605189501      (Schertz, TX)   \n",
       "1                   1.593817e+15  1593816671760502      (Schertz, TX)   \n",
       "2                   1.593360e+15  1593816307373967    (San Diego, CA)   \n",
       "3                   1.593816e+15  1593817141171761    (San Diego, CA)   \n",
       "4                   1.593817e+15  1593817173411564    (San Diego, CA)   \n",
       "...                          ...               ...                ...   \n",
       "607162                       NaN  1593813352318609        (Kirby, TX)   \n",
       "607163                       NaN  1593813431096000       (Austin, TX)   \n",
       "607164                       NaN  1593813539955034      (Leawood, KS)   \n",
       "607165                       NaN  1593813553558908    (Corsicana, TX)   \n",
       "607166                       NaN  1593813556338287  (Taylorville, IL)   \n",
       "\n",
       "                                                    items traffic_source  \\\n",
       "0                                                      []          email   \n",
       "1       [(NEWBED10, M_PREM_T, Premium Twin Mattress, 9...          email   \n",
       "2                                                      []          email   \n",
       "3       [(NEWBED10, M_STAN_T, Standard Twin Mattress, ...          email   \n",
       "4       [(NEWBED10, M_STAN_T, Standard Twin Mattress, ...          email   \n",
       "...                                                   ...            ...   \n",
       "607162                                                 []         google   \n",
       "607163                                                 []         google   \n",
       "607164                                                 []      instagram   \n",
       "607165                                                 []         google   \n",
       "607166                                                 []         google   \n",
       "\n",
       "        user_first_touch_timestamp  hour                  createdAt  \\\n",
       "0                 1593353545834835    22 2020-07-03 22:50:05.189501   \n",
       "1                 1593353545834835    22 2020-07-03 22:51:11.760502   \n",
       "2                 1593360175850522    22 2020-07-03 22:45:07.373967   \n",
       "3                 1593360175850522    22 2020-07-03 22:59:01.171761   \n",
       "4                 1593360175850522    22 2020-07-03 22:59:33.411564   \n",
       "...                            ...   ...                        ...   \n",
       "607162            1593813352318609    21 2020-07-03 21:55:52.318609   \n",
       "607163            1593813431096000    21 2020-07-03 21:57:11.096000   \n",
       "607164            1593813539955034    21 2020-07-03 21:58:59.955034   \n",
       "607165            1593813553558908    21 2020-07-03 21:59:13.558908   \n",
       "607166            1593813556338287    21 2020-07-03 21:59:16.338287   \n",
       "\n",
       "                              email  \n",
       "0       williamsmichael43@gmail.com  \n",
       "1       williamsmichael43@gmail.com  \n",
       "2            lindsey32@sanchez.info  \n",
       "3            lindsey32@sanchez.info  \n",
       "4            lindsey32@sanchez.info  \n",
       "...                             ...  \n",
       "607162                         None  \n",
       "607163                         None  \n",
       "607164                         None  \n",
       "607165                         None  \n",
       "607166                         None  \n",
       "\n",
       "[607167 rows x 13 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.format(\"delta\").load(\"spark-warehouse/join_static/\")\n",
    "\n",
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming-Streaming Joins\n",
    "Unlike streaming-static joins, where a real-time stream is enriched with a static dataset, streaming-streaming joins involve combining two real-time streams. This allows us to correlate events from different sources that are continuously updating.\n",
    "\n",
    "In this section, we read both the event and user data as streams, enabling a fully dynamic join that updates as new data arrives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da8f8dbd-c601-4ed6-b96a-0af6abbabf36",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# let's read in users dataframe as a stream\n",
    "\n",
    "# since we have created the dataframe from this data, we can cheat on getting the schema. Possible in development/debugging, not possible or recommended in production\n",
    "users_schema = users_df.schema\n",
    "\n",
    "users_stream_df = (spark\n",
    "                   .readStream\n",
    "                   .format(\"parquet\")\n",
    "                   .schema(users_schema)\n",
    "                   .option(\"maxFilesPerTrigger\", 1)\n",
    "                   .parquet(\"input/users.parquet\")\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eab5049a-272a-468b-a80a-0b25830b93fe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data exists\n"
     ]
    }
   ],
   "source": [
    "# let's do a stream to stream join\n",
    "\n",
    "joined_streams_df = (events_df\n",
    "            .join(users_stream_df.drop(\"user_first_touch_timestamp\"), \"user_id\")\n",
    "            )\n",
    "\n",
    "checkpoint_path = \"streaming/join_stream/_checkpoint\" \n",
    "table_name = \"join_stream\"\n",
    "output_path = f\"spark-warehouse/{table_name}\"\n",
    "\n",
    "join_stream_query = (joined_df\n",
    "  .writeStream\n",
    "  .outputMode(\"append\")\n",
    "  .format(\"delta\")\n",
    "  .queryName(\"join_stream_query\")\n",
    "  .trigger(processingTime=\"10 second\")\n",
    "  .option(\"checkpointLocation\", checkpoint_path)\n",
    "  .start(output_path) \n",
    ")\n",
    "\n",
    "create_table_if_exists(output_path,table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>device</th>\n",
       "      <th>ecommerce</th>\n",
       "      <th>event_name</th>\n",
       "      <th>event_previous_timestamp</th>\n",
       "      <th>event_timestamp</th>\n",
       "      <th>geo</th>\n",
       "      <th>items</th>\n",
       "      <th>traffic_source</th>\n",
       "      <th>user_first_touch_timestamp</th>\n",
       "      <th>hour</th>\n",
       "      <th>createdAt</th>\n",
       "      <th>email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UA000000105564540</td>\n",
       "      <td>iOS</td>\n",
       "      <td>(1075.5, 1, 1)</td>\n",
       "      <td>finalize</td>\n",
       "      <td>1.593775e+15</td>\n",
       "      <td>1593774992380669</td>\n",
       "      <td>(Indio, CA)</td>\n",
       "      <td>[(NEWBED10, M_STAN_K, Standard King Mattress, ...</td>\n",
       "      <td>email</td>\n",
       "      <td>1593277246048925</td>\n",
       "      <td>11</td>\n",
       "      <td>2020-07-03 11:16:32.380669</td>\n",
       "      <td>durhampeter@gomez-houston.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UA000000105564540</td>\n",
       "      <td>iOS</td>\n",
       "      <td>(None, None, None)</td>\n",
       "      <td>cc_info</td>\n",
       "      <td>1.593775e+15</td>\n",
       "      <td>1593774944098915</td>\n",
       "      <td>(Indio, CA)</td>\n",
       "      <td>[(NEWBED10, M_STAN_K, Standard King Mattress, ...</td>\n",
       "      <td>email</td>\n",
       "      <td>1593277246048925</td>\n",
       "      <td>11</td>\n",
       "      <td>2020-07-03 11:15:44.098915</td>\n",
       "      <td>durhampeter@gomez-houston.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UA000000105564540</td>\n",
       "      <td>iOS</td>\n",
       "      <td>(None, None, None)</td>\n",
       "      <td>shipping_info</td>\n",
       "      <td>1.593773e+15</td>\n",
       "      <td>1593774779043487</td>\n",
       "      <td>(Indio, CA)</td>\n",
       "      <td>[(NEWBED10, M_STAN_K, Standard King Mattress, ...</td>\n",
       "      <td>email</td>\n",
       "      <td>1593277246048925</td>\n",
       "      <td>11</td>\n",
       "      <td>2020-07-03 11:12:59.043487</td>\n",
       "      <td>durhampeter@gomez-houston.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UA000000105775637</td>\n",
       "      <td>Android</td>\n",
       "      <td>(None, None, None)</td>\n",
       "      <td>mattresses</td>\n",
       "      <td>1.593353e+15</td>\n",
       "      <td>1593775861745448</td>\n",
       "      <td>(Atlanta, GA)</td>\n",
       "      <td>[]</td>\n",
       "      <td>email</td>\n",
       "      <td>1593352019370286</td>\n",
       "      <td>11</td>\n",
       "      <td>2020-07-03 11:31:01.745448</td>\n",
       "      <td>ericanovak@yahoo.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UA000000105775637</td>\n",
       "      <td>Android</td>\n",
       "      <td>(None, None, None)</td>\n",
       "      <td>add_item</td>\n",
       "      <td>1.593776e+15</td>\n",
       "      <td>1593776070318936</td>\n",
       "      <td>(Atlanta, GA)</td>\n",
       "      <td>[(NEWBED10, M_STAN_K, Standard King Mattress, ...</td>\n",
       "      <td>email</td>\n",
       "      <td>1593352019370286</td>\n",
       "      <td>11</td>\n",
       "      <td>2020-07-03 11:34:30.318936</td>\n",
       "      <td>ericanovak@yahoo.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350775</th>\n",
       "      <td>UA000000107203756</td>\n",
       "      <td>Android</td>\n",
       "      <td>(None, None, None)</td>\n",
       "      <td>mattresses</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1593813352318609</td>\n",
       "      <td>(Kirby, TX)</td>\n",
       "      <td>[]</td>\n",
       "      <td>google</td>\n",
       "      <td>1593813352318609</td>\n",
       "      <td>21</td>\n",
       "      <td>2020-07-03 21:55:52.318609</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350776</th>\n",
       "      <td>UA000000107204051</td>\n",
       "      <td>Linux</td>\n",
       "      <td>(None, None, None)</td>\n",
       "      <td>mattresses</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1593813431096000</td>\n",
       "      <td>(Austin, TX)</td>\n",
       "      <td>[]</td>\n",
       "      <td>google</td>\n",
       "      <td>1593813431096000</td>\n",
       "      <td>21</td>\n",
       "      <td>2020-07-03 21:57:11.096000</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350777</th>\n",
       "      <td>UA000000107204375</td>\n",
       "      <td>Android</td>\n",
       "      <td>(None, None, None)</td>\n",
       "      <td>main</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1593813539955034</td>\n",
       "      <td>(Leawood, KS)</td>\n",
       "      <td>[]</td>\n",
       "      <td>instagram</td>\n",
       "      <td>1593813539955034</td>\n",
       "      <td>21</td>\n",
       "      <td>2020-07-03 21:58:59.955034</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350778</th>\n",
       "      <td>UA000000107204421</td>\n",
       "      <td>Android</td>\n",
       "      <td>(None, None, None)</td>\n",
       "      <td>main</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1593813553558908</td>\n",
       "      <td>(Corsicana, TX)</td>\n",
       "      <td>[]</td>\n",
       "      <td>google</td>\n",
       "      <td>1593813553558908</td>\n",
       "      <td>21</td>\n",
       "      <td>2020-07-03 21:59:13.558908</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350779</th>\n",
       "      <td>UA000000107204430</td>\n",
       "      <td>Android</td>\n",
       "      <td>(None, None, None)</td>\n",
       "      <td>main</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1593813556338287</td>\n",
       "      <td>(Taylorville, IL)</td>\n",
       "      <td>[]</td>\n",
       "      <td>google</td>\n",
       "      <td>1593813556338287</td>\n",
       "      <td>21</td>\n",
       "      <td>2020-07-03 21:59:16.338287</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>350780 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  user_id   device           ecommerce     event_name  \\\n",
       "0       UA000000105564540      iOS      (1075.5, 1, 1)       finalize   \n",
       "1       UA000000105564540      iOS  (None, None, None)        cc_info   \n",
       "2       UA000000105564540      iOS  (None, None, None)  shipping_info   \n",
       "3       UA000000105775637  Android  (None, None, None)     mattresses   \n",
       "4       UA000000105775637  Android  (None, None, None)       add_item   \n",
       "...                   ...      ...                 ...            ...   \n",
       "350775  UA000000107203756  Android  (None, None, None)     mattresses   \n",
       "350776  UA000000107204051    Linux  (None, None, None)     mattresses   \n",
       "350777  UA000000107204375  Android  (None, None, None)           main   \n",
       "350778  UA000000107204421  Android  (None, None, None)           main   \n",
       "350779  UA000000107204430  Android  (None, None, None)           main   \n",
       "\n",
       "        event_previous_timestamp   event_timestamp                geo  \\\n",
       "0                   1.593775e+15  1593774992380669        (Indio, CA)   \n",
       "1                   1.593775e+15  1593774944098915        (Indio, CA)   \n",
       "2                   1.593773e+15  1593774779043487        (Indio, CA)   \n",
       "3                   1.593353e+15  1593775861745448      (Atlanta, GA)   \n",
       "4                   1.593776e+15  1593776070318936      (Atlanta, GA)   \n",
       "...                          ...               ...                ...   \n",
       "350775                       NaN  1593813352318609        (Kirby, TX)   \n",
       "350776                       NaN  1593813431096000       (Austin, TX)   \n",
       "350777                       NaN  1593813539955034      (Leawood, KS)   \n",
       "350778                       NaN  1593813553558908    (Corsicana, TX)   \n",
       "350779                       NaN  1593813556338287  (Taylorville, IL)   \n",
       "\n",
       "                                                    items traffic_source  \\\n",
       "0       [(NEWBED10, M_STAN_K, Standard King Mattress, ...          email   \n",
       "1       [(NEWBED10, M_STAN_K, Standard King Mattress, ...          email   \n",
       "2       [(NEWBED10, M_STAN_K, Standard King Mattress, ...          email   \n",
       "3                                                      []          email   \n",
       "4       [(NEWBED10, M_STAN_K, Standard King Mattress, ...          email   \n",
       "...                                                   ...            ...   \n",
       "350775                                                 []         google   \n",
       "350776                                                 []         google   \n",
       "350777                                                 []      instagram   \n",
       "350778                                                 []         google   \n",
       "350779                                                 []         google   \n",
       "\n",
       "        user_first_touch_timestamp  hour                  createdAt  \\\n",
       "0                 1593277246048925    11 2020-07-03 11:16:32.380669   \n",
       "1                 1593277246048925    11 2020-07-03 11:15:44.098915   \n",
       "2                 1593277246048925    11 2020-07-03 11:12:59.043487   \n",
       "3                 1593352019370286    11 2020-07-03 11:31:01.745448   \n",
       "4                 1593352019370286    11 2020-07-03 11:34:30.318936   \n",
       "...                            ...   ...                        ...   \n",
       "350775            1593813352318609    21 2020-07-03 21:55:52.318609   \n",
       "350776            1593813431096000    21 2020-07-03 21:57:11.096000   \n",
       "350777            1593813539955034    21 2020-07-03 21:58:59.955034   \n",
       "350778            1593813553558908    21 2020-07-03 21:59:13.558908   \n",
       "350779            1593813556338287    21 2020-07-03 21:59:16.338287   \n",
       "\n",
       "                                email  \n",
       "0       durhampeter@gomez-houston.com  \n",
       "1       durhampeter@gomez-houston.com  \n",
       "2       durhampeter@gomez-houston.com  \n",
       "3                ericanovak@yahoo.com  \n",
       "4                ericanovak@yahoo.com  \n",
       "...                               ...  \n",
       "350775                           None  \n",
       "350776                           None  \n",
       "350777                           None  \n",
       "350778                           None  \n",
       "350779                           None  \n",
       "\n",
       "[350780 rows x 13 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.format(\"delta\").load(\"spark-warehouse/join_stream/\")\n",
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6ddcecc-b7c5-427e-bbd7-fea0f4886160",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for stream in spark.streams.active:\n",
    "  stream.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2ffb003-abb3-4689-9ad0-8564ac25e1cb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Further reading\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.window.html  \n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.withWatermark.html  \n",
    "https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html  \n",
    "https://docs.databricks.com/spark/latest/structured-streaming/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b127445-2b2f-49fb-84fe-fc9063f2450f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Task 1\n",
    "\n",
    "Create a streaming dataframe from the data in the following path:  \n",
    "`flights200701stream`\n",
    "\n",
    "The schema should contain\n",
    "* DepartureAt (timestamp)\n",
    "* UniqueCarrier (string)\n",
    "\n",
    "Process only 1 file per trigger.  \n",
    "\n",
    "Aggregate the data by count, using non-overlapping 30 minute windows.  \n",
    "Ignore any data that is older than 6 hours.\n",
    "\n",
    "The output should have 3 columns: startTime (window start time), UniqueCarrier, count.  \n",
    "\n",
    "Save to a delta table, firing the trigger every 5 seconds.\n",
    "\n",
    "Display the table, the output should be sorted ascending by startTime.\n",
    "\n",
    "Once the stream has produced some output, call the stream shutdown function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4204d127-eb1f-4c1f-be71-dd4c638935f2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Your answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "Join the Kafka streaming orders dataframe to the `product.csv` dataset.  \n",
    "Note that Spark assumes that any streaming dataframes refer to a directory, not a specific file.\n",
    "\n",
    "Aggregate the data by sum(price) (`total_price`) and a 2-minute tumbling window.  \n",
    "Add a 10 minute watermark, and store the data in a delta table.\n",
    "\n",
    "Create a view on top of the delta table with the following columns:\n",
    "* product_id\n",
    "* product_name\n",
    "* n_minus_2_window_total_price\n",
    "* n_minus_1_window_total_price\n",
    "* current_window_total_price\n",
    "\n",
    "The total_price columns need to be pivoted based on only the 3 most recent windows. \n",
    "The actual \n",
    "Order the dataset by descending `current_total_price`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Practice session - Structured Streaming continued",
   "notebookOrigID": 3621743426785043,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
