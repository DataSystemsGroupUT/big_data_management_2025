{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data exists\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from delta import *\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Create SparkSession with Delta Lake support\n",
    "builder = SparkSession.builder.appName(\"SensorDataWindow\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "# Define schema for the sensor data\n",
    "schema = StructType([\n",
    "    StructField(\"time\", IntegerType(), True),  # time is in epoch seconds\n",
    "    StructField(\"action\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Path to the folder containing the sensor data CSV files\n",
    "sensor_data_path = \"input/sensor-data/\"\n",
    "\n",
    "# Read the sensor data stream\n",
    "sensor_data_df = (spark.readStream\n",
    "                  .schema(schema)  # Define the schema explicitly\n",
    "                  .json(sensor_data_path)  # Read JSON files as a stream\n",
    ")\n",
    "\n",
    "# Create a timestamp column from the epoch time (if it's not already a timestamp)\n",
    "sensor_data_df = sensor_data_df.withColumn(\"timestamp\", F.from_unixtime(F.col(\"time\")))\n",
    "\n",
    "# Define the checkpoint and output path for Delta\n",
    "checkpoint_path = \"streaming/sensor_data_tumbling/_checkpoint\"\n",
    "output_path = \"delta/sensor_data_tumbling\"\n",
    "\n",
    "# Ensure the output path exists\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# Apply tumbling window (every 5 minutes) and count the number of actions in each window\n",
    "sensor_data_window_query = (sensor_data_df\n",
    "                            .groupBy(F.window(\"timestamp\", \"5 minutes\"), \"action\")  # Tumbling window on timestamp (every 5 minutes)\n",
    "                            .count()  # Count the number of actions in each window\n",
    "                            .withColumnRenamed(\"count\", \"action_count\")\n",
    "                            .writeStream\n",
    "                            .outputMode(\"complete\")  # Use complete mode to update the complete table in each trigger\n",
    "                            .format(\"delta\")\n",
    "                            .queryName(\"sensor_data_tumbling_query\")\n",
    "                            .trigger(processingTime=\"5 seconds\")  # Trigger every 5 seconds\n",
    "                            .option(\"checkpointLocation\", checkpoint_path)  # Checkpoint location\n",
    "                            .start(output_path)  # Delta table output path\n",
    ")\n",
    "\n",
    "# Function to create table if not exists\n",
    "def create_table_if_exists(output_path, table_name):\n",
    "    data_exists = False\n",
    "    for _i in range(60):  # Retry for 60 seconds\n",
    "        try:\n",
    "            time.sleep(1)\n",
    "            files = os.listdir(output_path)\n",
    "            for _f in files:\n",
    "                if \".parquet\" in _f:\n",
    "                    if len(os.listdir(f\"{output_path}/_delta_log\")) > 0:\n",
    "                        print(\"data exists\")\n",
    "                        data_exists = True\n",
    "                        break\n",
    "            if data_exists:\n",
    "                spark.sql(f\"CREATE TABLE IF NOT EXISTS {table_name} USING DELTA LOCATION '{output_path}'\")\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(e)  # Uncomment if you want to see exceptions\n",
    "            pass\n",
    "\n",
    "# Ensure table creation\n",
    "table_name = \"sensor_data_tumbling\"\n",
    "create_table_if_exists(output_path, table_name)\n",
    "\n",
    "# Wait for the streaming query to terminate\n",
    "sensor_data_window_query.awaitTermination(timeout = 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>window</th>\n",
       "      <th>action</th>\n",
       "      <th>action_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>(2016-07-26 02:45:00, 2016-07-26 02:50:00)</td>\n",
       "      <td>Open</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>(2016-07-26 02:50:00, 2016-07-26 02:55:00)</td>\n",
       "      <td>Open</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1104</th>\n",
       "      <td>(2016-07-26 02:50:00, 2016-07-26 02:55:00)</td>\n",
       "      <td>Close</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>(2016-07-26 02:55:00, 2016-07-26 03:00:00)</td>\n",
       "      <td>Open</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>859</th>\n",
       "      <td>(2016-07-26 02:55:00, 2016-07-26 03:00:00)</td>\n",
       "      <td>Close</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593</th>\n",
       "      <td>(2016-07-28 06:25:00, 2016-07-28 06:30:00)</td>\n",
       "      <td>Close</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1044</th>\n",
       "      <td>(2016-07-28 06:30:00, 2016-07-28 06:35:00)</td>\n",
       "      <td>Close</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>831</th>\n",
       "      <td>(2016-07-28 06:35:00, 2016-07-28 06:40:00)</td>\n",
       "      <td>Close</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(2016-07-28 06:40:00, 2016-07-28 06:45:00)</td>\n",
       "      <td>Close</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>(2016-07-28 06:45:00, 2016-07-28 06:50:00)</td>\n",
       "      <td>Close</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1226 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          window action  action_count\n",
       "56    (2016-07-26 02:45:00, 2016-07-26 02:50:00)   Open            32\n",
       "377   (2016-07-26 02:50:00, 2016-07-26 02:55:00)   Open            66\n",
       "1104  (2016-07-26 02:50:00, 2016-07-26 02:55:00)  Close             5\n",
       "641   (2016-07-26 02:55:00, 2016-07-26 03:00:00)   Open            81\n",
       "859   (2016-07-26 02:55:00, 2016-07-26 03:00:00)  Close             6\n",
       "...                                          ...    ...           ...\n",
       "593   (2016-07-28 06:25:00, 2016-07-28 06:30:00)  Close            14\n",
       "1044  (2016-07-28 06:30:00, 2016-07-28 06:35:00)  Close            15\n",
       "831   (2016-07-28 06:35:00, 2016-07-28 06:40:00)  Close            12\n",
       "9     (2016-07-28 06:40:00, 2016-07-28 06:45:00)  Close             3\n",
       "729   (2016-07-28 06:45:00, 2016-07-28 06:50:00)  Close             3\n",
       "\n",
       "[1226 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.format(\"delta\").load(\"delta/sensor_data_tumbling\")\n",
    "df.toPandas().sort_values(by=['window'], ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'delta/sensor_data_sliding/_delta_log'\n",
      "[Errno 2] No such file or directory: 'delta/sensor_data_sliding/_delta_log'\n",
      "[Errno 2] No such file or directory: 'delta/sensor_data_sliding/_delta_log'\n",
      "[Errno 2] No such file or directory: 'delta/sensor_data_sliding/_delta_log'\n",
      "[Errno 2] No such file or directory: 'delta/sensor_data_sliding/_delta_log'\n",
      "[Errno 2] No such file or directory: 'delta/sensor_data_sliding/_delta_log'\n",
      "data exists\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from delta import *\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Create SparkSession with Delta Lake support\n",
    "builder = SparkSession.builder.appName(\"SensorDataWindow\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "# Define schema for the sensor data\n",
    "schema = StructType([\n",
    "    StructField(\"time\", IntegerType(), True),  # time is in epoch seconds\n",
    "    StructField(\"action\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Path to the folder containing the sensor data CSV files\n",
    "sensor_data_path = \"input/sensor-data/\"\n",
    "\n",
    "# Read the sensor data stream\n",
    "sensor_data_df = (spark.readStream\n",
    "                  .schema(schema)  # Define the schema explicitly\n",
    "                  .json(sensor_data_path)  # Read JSON files as a stream\n",
    ")\n",
    "\n",
    "# Create a timestamp column from the epoch time (if it's not already a timestamp)\n",
    "sensor_data_df = sensor_data_df.withColumn(\"timestamp\", F.from_unixtime(F.col(\"time\")))\n",
    "\n",
    "# Define the checkpoint and output path for Delta\n",
    "checkpoint_path = \"streaming/sensor_data_sliding/_checkpoint\"\n",
    "output_path = \"delta/sensor_data_sliding\"\n",
    "\n",
    "# Ensure the output path exists\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# Apply tumbling window (every 5 minutes) and count the number of actions in each window\n",
    "sensor_data_window_query = (sensor_data_df\n",
    "                            .groupBy(F.window(\"timestamp\", \"5 minutes\", \"1 minute\"), \"action\")  # Tumbling window on timestamp (every 5 minutes)\n",
    "                            .count()  # Count the number of actions in each window\n",
    "                            .withColumnRenamed(\"count\", \"action_count\")\n",
    "                            .writeStream\n",
    "                            .outputMode(\"complete\")  # Use complete mode to update the complete table in each trigger\n",
    "                            .format(\"delta\")\n",
    "                            .queryName(\"sensor_data_tumbling_query\")\n",
    "                            .trigger(processingTime=\"5 seconds\")  # Trigger every 5 seconds\n",
    "                            .option(\"checkpointLocation\", checkpoint_path)  # Checkpoint location\n",
    "                            .start(output_path)  # Delta table output path\n",
    ")\n",
    "\n",
    "# Function to create table if not exists\n",
    "def create_table_if_exists(output_path, table_name):\n",
    "    data_exists = False\n",
    "    for _i in range(60):  # Retry for 60 seconds\n",
    "        try:\n",
    "            time.sleep(1)\n",
    "            files = os.listdir(output_path)\n",
    "            for _f in files:\n",
    "                if \".parquet\" in _f:\n",
    "                    if len(os.listdir(f\"{output_path}/_delta_log\")) > 0:\n",
    "                        print(\"data exists\")\n",
    "                        data_exists = True\n",
    "                        break\n",
    "            if data_exists:\n",
    "                spark.sql(f\"CREATE TABLE IF NOT EXISTS {table_name} USING DELTA LOCATION '{output_path}'\")\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(e)  # Uncomment if you want to see exceptions\n",
    "            pass\n",
    "\n",
    "# Ensure table creation\n",
    "table_name = \"sensor_data_sliding\"\n",
    "create_table_if_exists(output_path, table_name)\n",
    "\n",
    "# Wait for the streaming query to terminate\n",
    "sensor_data_window_query.awaitTermination(timeout = 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>window</th>\n",
       "      <th>action</th>\n",
       "      <th>action_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3352</th>\n",
       "      <td>(2016-07-26 02:41:00, 2016-07-26 02:46:00)</td>\n",
       "      <td>Open</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2324</th>\n",
       "      <td>(2016-07-26 02:42:00, 2016-07-26 02:47:00)</td>\n",
       "      <td>Open</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4249</th>\n",
       "      <td>(2016-07-26 02:43:00, 2016-07-26 02:48:00)</td>\n",
       "      <td>Open</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2726</th>\n",
       "      <td>(2016-07-26 02:44:00, 2016-07-26 02:49:00)</td>\n",
       "      <td>Open</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>(2016-07-26 02:45:00, 2016-07-26 02:50:00)</td>\n",
       "      <td>Open</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>966</th>\n",
       "      <td>(2016-07-28 06:44:00, 2016-07-28 06:49:00)</td>\n",
       "      <td>Close</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2283</th>\n",
       "      <td>(2016-07-28 06:45:00, 2016-07-28 06:50:00)</td>\n",
       "      <td>Close</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4967</th>\n",
       "      <td>(2016-07-28 06:46:00, 2016-07-28 06:51:00)</td>\n",
       "      <td>Close</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>657</th>\n",
       "      <td>(2016-07-28 06:47:00, 2016-07-28 06:52:00)</td>\n",
       "      <td>Close</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2924</th>\n",
       "      <td>(2016-07-28 06:48:00, 2016-07-28 06:53:00)</td>\n",
       "      <td>Close</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6136 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          window action  action_count\n",
       "3352  (2016-07-26 02:41:00, 2016-07-26 02:46:00)   Open             2\n",
       "2324  (2016-07-26 02:42:00, 2016-07-26 02:47:00)   Open             4\n",
       "4249  (2016-07-26 02:43:00, 2016-07-26 02:48:00)   Open            15\n",
       "2726  (2016-07-26 02:44:00, 2016-07-26 02:49:00)   Open            22\n",
       "217   (2016-07-26 02:45:00, 2016-07-26 02:50:00)   Open            32\n",
       "...                                          ...    ...           ...\n",
       "966   (2016-07-28 06:44:00, 2016-07-28 06:49:00)  Close             3\n",
       "2283  (2016-07-28 06:45:00, 2016-07-28 06:50:00)  Close             3\n",
       "4967  (2016-07-28 06:46:00, 2016-07-28 06:51:00)  Close             2\n",
       "657   (2016-07-28 06:47:00, 2016-07-28 06:52:00)  Close             2\n",
       "2924  (2016-07-28 06:48:00, 2016-07-28 06:53:00)  Close             1\n",
       "\n",
       "[6136 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.format(\"delta\").load(\"delta/sensor_data_sliding\")\n",
    "df.toPandas().sort_values(by=['window'], ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8307cf20-40d9-4eef-9d0b-af50f5901b50",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Structured Streaming\n",
    "  \n",
    "  \n",
    "* Kafka \n",
    "* Aggregations\n",
    "* Time windows\n",
    "* Watermarking\n",
    "* Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "475b45b8-9b3d-412e-9da5-0291489a5214",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Time windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d432333a-1335-4fc4-8b5d-1e79f16ebbf9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# commonly, you might not want an aggregation of a stream's whole history.\n",
    "# for this purpose, let's use window from functions - NB this is \"time windows\" not \"SQL-like (row) window function\"\n",
    "\n",
    "checkpoint_path = \"streaming/orders_most_products_tumb/_checkpoint\" \n",
    "table_name = \"orders_most_products_tumb\"\n",
    "output_path = f\"spark-warehouse/{table_name}\"\n",
    "\n",
    "orders_most_products_tumb_query = (orders_cleaned_df\n",
    "  .select(\"user_id\",F.size(\"product_ids\").alias(\"count_of_products\"), \"order_timestamp\")\n",
    "  .groupBy(\"user_id\", F.window(\"order_timestamp\", \"5 minute\")) # Aggregate by user, every 5 minute block. This is a \"tumbling window\"\n",
    "  .sum(\"count_of_products\")\n",
    "  .withColumnRenamed(\"sum(count_of_products)\", \"product_count\")\n",
    "  .writeStream\n",
    "  .outputMode(\"complete\") # we overwrite the complete table with every trigger\n",
    "  .format(\"delta\")\n",
    "  .queryName(\"orders_most_products_tumb_query\")\n",
    "  .trigger(processingTime=\"5 second\")\n",
    "  .option(\"checkpointLocation\", checkpoint_path)\n",
    "  .start(output_path) \n",
    ")\n",
    "\n",
    "create_table_if_exists(output_path,table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_table_if_exists(output_path,table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.table(table_name).orderBy(F.desc(\"window.end\"),F.desc(\"product_count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e86c785c-51ac-4780-be1e-184e01fee543",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# if we want to keep always the latest time window then we can use sliding windows\n",
    "\n",
    "checkpoint_path = \"streaming/orders_most_products_slide/_checkpoint\" \n",
    "table_name = \"orders_most_products_slide\"\n",
    "output_path = f\"spark-warehouse/{table_name}\"\n",
    "\n",
    "orders_most_products_slide_query = (orders_cleaned_df\n",
    "  .select(\"user_id\",F.size(\"product_ids\").alias(\"count_of_products\"), \"order_timestamp\")\n",
    "  .groupBy(\"user_id\", F.window(\"order_timestamp\", \"5 minute\", \"1 minute\")) # Aggregate by user, every 5 minute block sliding by 1 minute.\n",
    "  .sum(\"count_of_products\")\n",
    "  .withColumnRenamed(\"sum(count_of_products)\", \"product_count\")\n",
    "  .writeStream\n",
    "  .outputMode(\"complete\") # we overwrite the complete table with every trigger\n",
    "  .format(\"delta\")\n",
    "  .queryName(\"orders_most_products_slide_query\")\n",
    "  .trigger(processingTime=\"5 second\")\n",
    "  .option(\"checkpointLocation\", checkpoint_path)\n",
    "  .start(output_path) \n",
    ")\n",
    "\n",
    "create_table_if_exists(output_path,table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.table(table_name).orderBy(F.desc(\"window.end\"),F.desc(\"product_count\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "051067d9-2417-4562-bf14-5f55b0347200",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Non-Kafka part starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acac1e02-d555-4fdc-aa57-0eb83b64c55d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# let's try with a different, simpler dataset\n",
    "\n",
    "input_path = \"input/sensor-data\"\n",
    "\n",
    "json_schema = \"time timestamp, action string\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5f36054-4522-42b8-97aa-7fabeede5da0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Let's create a dataframe and apply some transformations and aggregation\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m input_df \u001b[38;5;241m=\u001b[39m (\u001b[43mspark\u001b[49m\n\u001b[1;32m      4\u001b[0m   \u001b[38;5;241m.\u001b[39mreadStream                                 \n\u001b[1;32m      5\u001b[0m   \u001b[38;5;241m.\u001b[39mschema(json_schema)                       \n\u001b[1;32m      6\u001b[0m   \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaxFilesPerTrigger\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)            \n\u001b[1;32m      7\u001b[0m   \u001b[38;5;241m.\u001b[39mjson(input_path)                           \n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m     10\u001b[0m counts_df \u001b[38;5;241m=\u001b[39m (input_df\n\u001b[1;32m     11\u001b[0m   \u001b[38;5;241m.\u001b[39mgroupBy(F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction\u001b[39m\u001b[38;5;124m\"\u001b[39m),                     \u001b[38;5;66;03m# Aggregate by action\u001b[39;00m\n\u001b[1;32m     12\u001b[0m            F\u001b[38;5;241m.\u001b[39mwindow(F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1 hour\u001b[39m\u001b[38;5;124m\"\u001b[39m))     \u001b[38;5;66;03m# and by a 1 hour window\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m           F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction\u001b[39m\u001b[38;5;124m\"\u001b[39m))                      \n\u001b[1;32m     17\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# Let's create a dataframe and apply some transformations and aggregation\n",
    "\n",
    "input_df = (spark\n",
    "  .readStream                                 \n",
    "  .schema(json_schema)                       \n",
    "  .option(\"maxFilesPerTrigger\", 1)            \n",
    "  .json(input_path)                           \n",
    ")\n",
    "\n",
    "counts_df = (input_df\n",
    "  .groupBy(F.col(\"action\"),                     # Aggregate by action\n",
    "           F.window(F.col(\"time\"), \"1 hour\"))     # and by a 1 hour window\n",
    "  .count()                                    # Count the actions\n",
    "  .select(F.col(\"window.start\").alias(\"start\"), \n",
    "          F.col(\"count\"),                       \n",
    "          F.col(\"action\"))                      \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aea650a6-ed4e-44a2-8eb7-65d73326a046",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"streaming/counts/_checkpoint\" \n",
    "table_name = \"counts\"\n",
    "output_path = f\"spark-warehouse/{table_name}\"\n",
    "\n",
    "counts_query = (counts_df\n",
    "  .writeStream\n",
    "  .outputMode(\"complete\")\n",
    "  .format(\"delta\")\n",
    "  .queryName(\"counts_query\")\n",
    "  .trigger(processingTime=\"10 second\")\n",
    "  .option(\"checkpointLocation\", checkpoint_path)\n",
    "  .start(output_path) \n",
    ")\n",
    "\n",
    "create_table_if_exists(output_path,table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.table(table_name).orderBy(F.col(\"start\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e200d92c-a0ce-4b7c-be79-3658e2e56dd5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Watermarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d968b6b-01a9-480a-8d63-b3e5c0ca159a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# in actual use cases, the queries above would keep running for a very long time and the amount of windows would grow indefinitely\n",
    "# keeping track of all the states puts pressure on memory\n",
    "# also it may often be irrelevant if delayed data updates our figures\n",
    "\n",
    "watermarked_stream = \"watermarked_stream\"\n",
    "\n",
    "watermarked_df = (input_df\n",
    "  .withWatermark(\"time\", \"2 hours\")             # Specify a 2-hour watermark\n",
    "  .groupBy(F.col(\"action\"),                       # Aggregate by action...\n",
    "           F.window(F.col(\"time\"), \"1 hour\"))       # ...then by a 1 hour window\n",
    "  .count()                                      # For each aggregate, produce a count\n",
    "  .select(F.col(\"window.start\").alias(\"start\"),   # Elevate field to column\n",
    "          F.col(\"count\"),                         # Include count\n",
    "          F.col(\"action\"))                        # Include action\n",
    ")\n",
    "display(watermarked_df, streamName = watermarked_stream) # Start the stream and display it\n",
    "\n",
    "# important note: watermarking guarantees that any event within the window gets in. It does not guarantee leaving anything out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in actual use cases, the queries above would keep running for a very long time and the amount of windows would grow indefinitely\n",
    "# keeping track of all the states puts pressure on memory\n",
    "# also it may often be irrelevant if delayed data updates our figures\n",
    "\n",
    "checkpoint_path = \"streaming/orders_most_products_slide_wm/_checkpoint\" \n",
    "table_name = \"orders_most_products_slide_wm\"\n",
    "output_path = f\"spark-warehouse/{table_name}\"\n",
    "\n",
    "orders_most_products_slide_wm_query = (orders_cleaned_df\n",
    "  .withWatermark(\"order_timestamp\", \"20 minute\")             # Specify a 20-minute watermark\n",
    "  .select(\"user_id\",F.size(\"product_ids\").alias(\"count_of_products\"), \"order_timestamp\")\n",
    "  .groupBy(\"user_id\", F.window(\"order_timestamp\", \"5 minute\", \"1 minute\")) # Aggregate by user, every 5 minute block sliding by 1 minute.\n",
    "  .sum(\"count_of_products\")\n",
    "  .withColumnRenamed(\"sum(count_of_products)\", \"product_count\")\n",
    "  .writeStream\n",
    "  .outputMode(\"complete\") # we overwrite the complete table with every trigger\n",
    "  .format(\"delta\")\n",
    "  .queryName(\"orders_most_products_slide_wm_query\")\n",
    "  .trigger(processingTime=\"5 second\")\n",
    "  .option(\"checkpointLocation\", checkpoint_path)\n",
    "  .start(output_path) \n",
    ")\n",
    "\n",
    "create_table_if_exists(output_path,table_name)\n",
    "\n",
    "# important note: watermarking guarantees that any event within the window gets in. It does not guarantee leaving anything out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.table(table_name).orderBy(F.desc(\"window.end\"),F.desc(\"product_count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf4aa920-978f-4e07-ab18-efca64d10f51",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's import another dataset. Let's say we are interested in hourly monitoring of incoming traffic to our website\n",
    "\n",
    "schema = \"device STRING, ecommerce STRUCT<purchase_revenue_in_usd: DOUBLE, total_item_quantity: BIGINT, unique_items: BIGINT>, event_name STRING, event_previous_timestamp BIGINT, event_timestamp BIGINT, geo STRUCT<city: STRING, state: STRING>, items ARRAY<STRUCT<coupon: STRING, item_id: STRING, item_name: STRING, item_revenue_in_usd: DOUBLE, price_in_usd: DOUBLE, quantity: BIGINT>>, traffic_source STRING, user_first_touch_timestamp BIGINT, user_id STRING\"\n",
    "\n",
    "hourlyEventsPath = \"input/events20200703\"\n",
    "\n",
    "website_df = (spark.readStream\n",
    "  .schema(schema)\n",
    "  .option(\"maxFilesPerTrigger\", 1)\n",
    "  .json(hourlyEventsPath)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81d948ed-3a26-45e7-8e92-836cc07ae9c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# this dataframe does not have a proper timestamp column. So we need to create one and use it for watermarking\n",
    "\n",
    "events_df = (website_df\n",
    "             .withColumn(\"createdAt\", (F.col(\"event_timestamp\") / 1e6).cast(\"timestamp\"))\n",
    "             .withWatermark(\"createdAt\", \"2 hours\")\n",
    ")             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb7b62a6-8a6d-41bc-a226-5c5d7cf40943",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data exists\n"
     ]
    }
   ],
   "source": [
    "# now we can do an aggregation\n",
    "\n",
    "traffic_df = (events_df\n",
    "             .groupBy(\"traffic_source\"\n",
    "                      , F.window(F.col(\"createdAt\"), \"1 hour\"))\n",
    "             .agg(F.approx_count_distinct(\"user_id\").alias(\"active_users\"))\n",
    "             .select(F.col(\"traffic_source\")\n",
    "                     , F.col(\"active_users\")\n",
    "                     , F.hour(F.col(\"window.start\")).alias(\"hour\"))\n",
    "             .sort(\"hour\")\n",
    ")\n",
    "\n",
    "\n",
    "checkpoint_path = \"streaming/traffic/_checkpoint\" \n",
    "table_name = \"traffic\"\n",
    "output_path = f\"spark-warehouse/{table_name}\"\n",
    "\n",
    "traffic_query = (traffic_df\n",
    "  .writeStream\n",
    "  .outputMode(\"complete\") # we overwrite the complete table with every trigger\n",
    "  .format(\"delta\")\n",
    "  .queryName(\"traffic_query\")\n",
    "  .trigger(processingTime=\"10 second\")\n",
    "  .option(\"checkpointLocation\", checkpoint_path)\n",
    "  .start(output_path) \n",
    ")\n",
    "\n",
    "create_table_if_exists(output_path,table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "307411ab-59c4-4026-b6d8-65b745ca1d28",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Joining streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d2d54db-d035-4163-a9af-679884a3edb1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# let's load in users dataset\n",
    "\n",
    "users_df = spark.read.parquet(\"input/users.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "600645dc-8cb3-47ba-8d28-bc6330cefdb0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "Cannot start query with name join_static_query as a query with that name is already active in this SparkSession",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 19\u001b[0m\n\u001b[1;32m      9\u001b[0m table_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjoin_static\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark-warehouse/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     12\u001b[0m join_static_query \u001b[38;5;241m=\u001b[39m (\u001b[43mjoined_df\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriteStream\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputMode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mappend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdelta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqueryName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjoin_static_query\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrigger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessingTime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m10 second\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcheckpointLocation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m---> 19\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     22\u001b[0m create_table_if_exists(output_path,table_name)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/streaming/readwriter.py:1529\u001b[0m, in \u001b[0;36mDataStreamWriter.start\u001b[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001b[0m\n\u001b[1;32m   1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sq(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39mstart())\n\u001b[1;32m   1528\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1529\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sq(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: Cannot start query with name join_static_query as a query with that name is already active in this SparkSession"
     ]
    }
   ],
   "source": [
    "# join works same way as with regular dataframes.\n",
    "# note: this is streaming<->static join\n",
    "\n",
    "joined_df = (events_df\n",
    "            .join(users_df.drop(\"user_first_touch_timestamp\"), \"user_id\")\n",
    "            )\n",
    "\n",
    "checkpoint_path = \"streaming/join_static/_checkpoint\" \n",
    "table_name = \"join_static\"\n",
    "output_path = f\"spark-warehouse/{table_name}\"\n",
    "\n",
    "join_static_query = (joined_df\n",
    "  .writeStream\n",
    "  .outputMode(\"append\")\n",
    "  .format(\"delta\")\n",
    "  .queryName(\"join_static_query\")\n",
    "  .trigger(processingTime=\"10 second\")\n",
    "  .option(\"checkpointLocation\", checkpoint_path)\n",
    "  .start(output_path) \n",
    ")\n",
    "\n",
    "create_table_if_exists(output_path,table_name)\n",
    "join_static_query.awaitTermination(timeout = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>device</th>\n",
       "      <th>ecommerce</th>\n",
       "      <th>event_name</th>\n",
       "      <th>event_previous_timestamp</th>\n",
       "      <th>event_timestamp</th>\n",
       "      <th>geo</th>\n",
       "      <th>items</th>\n",
       "      <th>traffic_source</th>\n",
       "      <th>user_first_touch_timestamp</th>\n",
       "      <th>hour</th>\n",
       "      <th>createdAt</th>\n",
       "      <th>email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UA000000105784116</td>\n",
       "      <td>macOS</td>\n",
       "      <td>(None, None, None)</td>\n",
       "      <td>mattresses</td>\n",
       "      <td>1.593356e+15</td>\n",
       "      <td>1593816605189501</td>\n",
       "      <td>(Schertz, TX)</td>\n",
       "      <td>[]</td>\n",
       "      <td>email</td>\n",
       "      <td>1593353545834835</td>\n",
       "      <td>22</td>\n",
       "      <td>2020-07-03 22:50:05.189501</td>\n",
       "      <td>williamsmichael43@gmail.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UA000000105784116</td>\n",
       "      <td>macOS</td>\n",
       "      <td>(None, None, None)</td>\n",
       "      <td>add_item</td>\n",
       "      <td>1.593817e+15</td>\n",
       "      <td>1593816671760502</td>\n",
       "      <td>(Schertz, TX)</td>\n",
       "      <td>[(NEWBED10, M_PREM_T, Premium Twin Mattress, 9...</td>\n",
       "      <td>email</td>\n",
       "      <td>1593353545834835</td>\n",
       "      <td>22</td>\n",
       "      <td>2020-07-03 22:51:11.760502</td>\n",
       "      <td>williamsmichael43@gmail.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UA000000105826647</td>\n",
       "      <td>Windows</td>\n",
       "      <td>(None, None, None)</td>\n",
       "      <td>mattresses</td>\n",
       "      <td>1.593360e+15</td>\n",
       "      <td>1593816307373967</td>\n",
       "      <td>(San Diego, CA)</td>\n",
       "      <td>[]</td>\n",
       "      <td>email</td>\n",
       "      <td>1593360175850522</td>\n",
       "      <td>22</td>\n",
       "      <td>2020-07-03 22:45:07.373967</td>\n",
       "      <td>lindsey32@sanchez.info</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UA000000105826647</td>\n",
       "      <td>Windows</td>\n",
       "      <td>(None, None, None)</td>\n",
       "      <td>add_item</td>\n",
       "      <td>1.593816e+15</td>\n",
       "      <td>1593817141171761</td>\n",
       "      <td>(San Diego, CA)</td>\n",
       "      <td>[(NEWBED10, M_STAN_T, Standard Twin Mattress, ...</td>\n",
       "      <td>email</td>\n",
       "      <td>1593360175850522</td>\n",
       "      <td>22</td>\n",
       "      <td>2020-07-03 22:59:01.171761</td>\n",
       "      <td>lindsey32@sanchez.info</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UA000000105826647</td>\n",
       "      <td>Windows</td>\n",
       "      <td>(None, None, None)</td>\n",
       "      <td>cart</td>\n",
       "      <td>1.593817e+15</td>\n",
       "      <td>1593817173411564</td>\n",
       "      <td>(San Diego, CA)</td>\n",
       "      <td>[(NEWBED10, M_STAN_T, Standard Twin Mattress, ...</td>\n",
       "      <td>email</td>\n",
       "      <td>1593360175850522</td>\n",
       "      <td>22</td>\n",
       "      <td>2020-07-03 22:59:33.411564</td>\n",
       "      <td>lindsey32@sanchez.info</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607162</th>\n",
       "      <td>UA000000107203756</td>\n",
       "      <td>Android</td>\n",
       "      <td>(None, None, None)</td>\n",
       "      <td>mattresses</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1593813352318609</td>\n",
       "      <td>(Kirby, TX)</td>\n",
       "      <td>[]</td>\n",
       "      <td>google</td>\n",
       "      <td>1593813352318609</td>\n",
       "      <td>21</td>\n",
       "      <td>2020-07-03 21:55:52.318609</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607163</th>\n",
       "      <td>UA000000107204051</td>\n",
       "      <td>Linux</td>\n",
       "      <td>(None, None, None)</td>\n",
       "      <td>mattresses</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1593813431096000</td>\n",
       "      <td>(Austin, TX)</td>\n",
       "      <td>[]</td>\n",
       "      <td>google</td>\n",
       "      <td>1593813431096000</td>\n",
       "      <td>21</td>\n",
       "      <td>2020-07-03 21:57:11.096000</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607164</th>\n",
       "      <td>UA000000107204375</td>\n",
       "      <td>Android</td>\n",
       "      <td>(None, None, None)</td>\n",
       "      <td>main</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1593813539955034</td>\n",
       "      <td>(Leawood, KS)</td>\n",
       "      <td>[]</td>\n",
       "      <td>instagram</td>\n",
       "      <td>1593813539955034</td>\n",
       "      <td>21</td>\n",
       "      <td>2020-07-03 21:58:59.955034</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607165</th>\n",
       "      <td>UA000000107204421</td>\n",
       "      <td>Android</td>\n",
       "      <td>(None, None, None)</td>\n",
       "      <td>main</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1593813553558908</td>\n",
       "      <td>(Corsicana, TX)</td>\n",
       "      <td>[]</td>\n",
       "      <td>google</td>\n",
       "      <td>1593813553558908</td>\n",
       "      <td>21</td>\n",
       "      <td>2020-07-03 21:59:13.558908</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607166</th>\n",
       "      <td>UA000000107204430</td>\n",
       "      <td>Android</td>\n",
       "      <td>(None, None, None)</td>\n",
       "      <td>main</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1593813556338287</td>\n",
       "      <td>(Taylorville, IL)</td>\n",
       "      <td>[]</td>\n",
       "      <td>google</td>\n",
       "      <td>1593813556338287</td>\n",
       "      <td>21</td>\n",
       "      <td>2020-07-03 21:59:16.338287</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>607167 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  user_id   device           ecommerce  event_name  \\\n",
       "0       UA000000105784116    macOS  (None, None, None)  mattresses   \n",
       "1       UA000000105784116    macOS  (None, None, None)    add_item   \n",
       "2       UA000000105826647  Windows  (None, None, None)  mattresses   \n",
       "3       UA000000105826647  Windows  (None, None, None)    add_item   \n",
       "4       UA000000105826647  Windows  (None, None, None)        cart   \n",
       "...                   ...      ...                 ...         ...   \n",
       "607162  UA000000107203756  Android  (None, None, None)  mattresses   \n",
       "607163  UA000000107204051    Linux  (None, None, None)  mattresses   \n",
       "607164  UA000000107204375  Android  (None, None, None)        main   \n",
       "607165  UA000000107204421  Android  (None, None, None)        main   \n",
       "607166  UA000000107204430  Android  (None, None, None)        main   \n",
       "\n",
       "        event_previous_timestamp   event_timestamp                geo  \\\n",
       "0                   1.593356e+15  1593816605189501      (Schertz, TX)   \n",
       "1                   1.593817e+15  1593816671760502      (Schertz, TX)   \n",
       "2                   1.593360e+15  1593816307373967    (San Diego, CA)   \n",
       "3                   1.593816e+15  1593817141171761    (San Diego, CA)   \n",
       "4                   1.593817e+15  1593817173411564    (San Diego, CA)   \n",
       "...                          ...               ...                ...   \n",
       "607162                       NaN  1593813352318609        (Kirby, TX)   \n",
       "607163                       NaN  1593813431096000       (Austin, TX)   \n",
       "607164                       NaN  1593813539955034      (Leawood, KS)   \n",
       "607165                       NaN  1593813553558908    (Corsicana, TX)   \n",
       "607166                       NaN  1593813556338287  (Taylorville, IL)   \n",
       "\n",
       "                                                    items traffic_source  \\\n",
       "0                                                      []          email   \n",
       "1       [(NEWBED10, M_PREM_T, Premium Twin Mattress, 9...          email   \n",
       "2                                                      []          email   \n",
       "3       [(NEWBED10, M_STAN_T, Standard Twin Mattress, ...          email   \n",
       "4       [(NEWBED10, M_STAN_T, Standard Twin Mattress, ...          email   \n",
       "...                                                   ...            ...   \n",
       "607162                                                 []         google   \n",
       "607163                                                 []         google   \n",
       "607164                                                 []      instagram   \n",
       "607165                                                 []         google   \n",
       "607166                                                 []         google   \n",
       "\n",
       "        user_first_touch_timestamp  hour                  createdAt  \\\n",
       "0                 1593353545834835    22 2020-07-03 22:50:05.189501   \n",
       "1                 1593353545834835    22 2020-07-03 22:51:11.760502   \n",
       "2                 1593360175850522    22 2020-07-03 22:45:07.373967   \n",
       "3                 1593360175850522    22 2020-07-03 22:59:01.171761   \n",
       "4                 1593360175850522    22 2020-07-03 22:59:33.411564   \n",
       "...                            ...   ...                        ...   \n",
       "607162            1593813352318609    21 2020-07-03 21:55:52.318609   \n",
       "607163            1593813431096000    21 2020-07-03 21:57:11.096000   \n",
       "607164            1593813539955034    21 2020-07-03 21:58:59.955034   \n",
       "607165            1593813553558908    21 2020-07-03 21:59:13.558908   \n",
       "607166            1593813556338287    21 2020-07-03 21:59:16.338287   \n",
       "\n",
       "                              email  \n",
       "0       williamsmichael43@gmail.com  \n",
       "1       williamsmichael43@gmail.com  \n",
       "2            lindsey32@sanchez.info  \n",
       "3            lindsey32@sanchez.info  \n",
       "4            lindsey32@sanchez.info  \n",
       "...                             ...  \n",
       "607162                         None  \n",
       "607163                         None  \n",
       "607164                         None  \n",
       "607165                         None  \n",
       "607166                         None  \n",
       "\n",
       "[607167 rows x 13 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.format(\"delta\").load(\"spark-warehouse/join_static/\")\n",
    "\n",
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da8f8dbd-c601-4ed6-b96a-0af6abbabf36",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# let's read in users dataframe as a stream\n",
    "\n",
    "# since we have created the dataframe from this data, we can cheat on getting the schema. Possible in development/debugging, not possible or recommended in production\n",
    "users_schema = users_df.schema\n",
    "\n",
    "users_stream_df = (spark\n",
    "                   .readStream\n",
    "                   .format(\"parquet\")\n",
    "                   .schema(users_schema)\n",
    "                   .option(\"maxFilesPerTrigger\", 1)\n",
    "                   .parquet(\"input/users.parquet\")\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eab5049a-272a-468b-a80a-0b25830b93fe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data exists\n"
     ]
    }
   ],
   "source": [
    "# let's do a stream to stream join\n",
    "\n",
    "joined_streams_df = (events_df\n",
    "            .join(users_stream_df.drop(\"user_first_touch_timestamp\"), \"user_id\")\n",
    "            )\n",
    "\n",
    "checkpoint_path = \"streaming/join_stream/_checkpoint\" \n",
    "table_name = \"join_stream\"\n",
    "output_path = f\"spark-warehouse/{table_name}\"\n",
    "\n",
    "join_stream_query = (joined_df\n",
    "  .writeStream\n",
    "  .outputMode(\"append\")\n",
    "  .format(\"delta\")\n",
    "  .queryName(\"join_stream_query\")\n",
    "  .trigger(processingTime=\"10 second\")\n",
    "  .option(\"checkpointLocation\", checkpoint_path)\n",
    "  .start(output_path) \n",
    ")\n",
    "\n",
    "create_table_if_exists(output_path,table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>device</th>\n",
       "      <th>ecommerce</th>\n",
       "      <th>event_name</th>\n",
       "      <th>event_previous_timestamp</th>\n",
       "      <th>event_timestamp</th>\n",
       "      <th>geo</th>\n",
       "      <th>items</th>\n",
       "      <th>traffic_source</th>\n",
       "      <th>user_first_touch_timestamp</th>\n",
       "      <th>hour</th>\n",
       "      <th>createdAt</th>\n",
       "      <th>email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UA000000105564540</td>\n",
       "      <td>iOS</td>\n",
       "      <td>(1075.5, 1, 1)</td>\n",
       "      <td>finalize</td>\n",
       "      <td>1.593775e+15</td>\n",
       "      <td>1593774992380669</td>\n",
       "      <td>(Indio, CA)</td>\n",
       "      <td>[(NEWBED10, M_STAN_K, Standard King Mattress, ...</td>\n",
       "      <td>email</td>\n",
       "      <td>1593277246048925</td>\n",
       "      <td>11</td>\n",
       "      <td>2020-07-03 11:16:32.380669</td>\n",
       "      <td>durhampeter@gomez-houston.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UA000000105564540</td>\n",
       "      <td>iOS</td>\n",
       "      <td>(None, None, None)</td>\n",
       "      <td>cc_info</td>\n",
       "      <td>1.593775e+15</td>\n",
       "      <td>1593774944098915</td>\n",
       "      <td>(Indio, CA)</td>\n",
       "      <td>[(NEWBED10, M_STAN_K, Standard King Mattress, ...</td>\n",
       "      <td>email</td>\n",
       "      <td>1593277246048925</td>\n",
       "      <td>11</td>\n",
       "      <td>2020-07-03 11:15:44.098915</td>\n",
       "      <td>durhampeter@gomez-houston.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UA000000105564540</td>\n",
       "      <td>iOS</td>\n",
       "      <td>(None, None, None)</td>\n",
       "      <td>shipping_info</td>\n",
       "      <td>1.593773e+15</td>\n",
       "      <td>1593774779043487</td>\n",
       "      <td>(Indio, CA)</td>\n",
       "      <td>[(NEWBED10, M_STAN_K, Standard King Mattress, ...</td>\n",
       "      <td>email</td>\n",
       "      <td>1593277246048925</td>\n",
       "      <td>11</td>\n",
       "      <td>2020-07-03 11:12:59.043487</td>\n",
       "      <td>durhampeter@gomez-houston.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UA000000105775637</td>\n",
       "      <td>Android</td>\n",
       "      <td>(None, None, None)</td>\n",
       "      <td>mattresses</td>\n",
       "      <td>1.593353e+15</td>\n",
       "      <td>1593775861745448</td>\n",
       "      <td>(Atlanta, GA)</td>\n",
       "      <td>[]</td>\n",
       "      <td>email</td>\n",
       "      <td>1593352019370286</td>\n",
       "      <td>11</td>\n",
       "      <td>2020-07-03 11:31:01.745448</td>\n",
       "      <td>ericanovak@yahoo.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UA000000105775637</td>\n",
       "      <td>Android</td>\n",
       "      <td>(None, None, None)</td>\n",
       "      <td>add_item</td>\n",
       "      <td>1.593776e+15</td>\n",
       "      <td>1593776070318936</td>\n",
       "      <td>(Atlanta, GA)</td>\n",
       "      <td>[(NEWBED10, M_STAN_K, Standard King Mattress, ...</td>\n",
       "      <td>email</td>\n",
       "      <td>1593352019370286</td>\n",
       "      <td>11</td>\n",
       "      <td>2020-07-03 11:34:30.318936</td>\n",
       "      <td>ericanovak@yahoo.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350775</th>\n",
       "      <td>UA000000107203756</td>\n",
       "      <td>Android</td>\n",
       "      <td>(None, None, None)</td>\n",
       "      <td>mattresses</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1593813352318609</td>\n",
       "      <td>(Kirby, TX)</td>\n",
       "      <td>[]</td>\n",
       "      <td>google</td>\n",
       "      <td>1593813352318609</td>\n",
       "      <td>21</td>\n",
       "      <td>2020-07-03 21:55:52.318609</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350776</th>\n",
       "      <td>UA000000107204051</td>\n",
       "      <td>Linux</td>\n",
       "      <td>(None, None, None)</td>\n",
       "      <td>mattresses</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1593813431096000</td>\n",
       "      <td>(Austin, TX)</td>\n",
       "      <td>[]</td>\n",
       "      <td>google</td>\n",
       "      <td>1593813431096000</td>\n",
       "      <td>21</td>\n",
       "      <td>2020-07-03 21:57:11.096000</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350777</th>\n",
       "      <td>UA000000107204375</td>\n",
       "      <td>Android</td>\n",
       "      <td>(None, None, None)</td>\n",
       "      <td>main</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1593813539955034</td>\n",
       "      <td>(Leawood, KS)</td>\n",
       "      <td>[]</td>\n",
       "      <td>instagram</td>\n",
       "      <td>1593813539955034</td>\n",
       "      <td>21</td>\n",
       "      <td>2020-07-03 21:58:59.955034</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350778</th>\n",
       "      <td>UA000000107204421</td>\n",
       "      <td>Android</td>\n",
       "      <td>(None, None, None)</td>\n",
       "      <td>main</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1593813553558908</td>\n",
       "      <td>(Corsicana, TX)</td>\n",
       "      <td>[]</td>\n",
       "      <td>google</td>\n",
       "      <td>1593813553558908</td>\n",
       "      <td>21</td>\n",
       "      <td>2020-07-03 21:59:13.558908</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350779</th>\n",
       "      <td>UA000000107204430</td>\n",
       "      <td>Android</td>\n",
       "      <td>(None, None, None)</td>\n",
       "      <td>main</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1593813556338287</td>\n",
       "      <td>(Taylorville, IL)</td>\n",
       "      <td>[]</td>\n",
       "      <td>google</td>\n",
       "      <td>1593813556338287</td>\n",
       "      <td>21</td>\n",
       "      <td>2020-07-03 21:59:16.338287</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>350780 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  user_id   device           ecommerce     event_name  \\\n",
       "0       UA000000105564540      iOS      (1075.5, 1, 1)       finalize   \n",
       "1       UA000000105564540      iOS  (None, None, None)        cc_info   \n",
       "2       UA000000105564540      iOS  (None, None, None)  shipping_info   \n",
       "3       UA000000105775637  Android  (None, None, None)     mattresses   \n",
       "4       UA000000105775637  Android  (None, None, None)       add_item   \n",
       "...                   ...      ...                 ...            ...   \n",
       "350775  UA000000107203756  Android  (None, None, None)     mattresses   \n",
       "350776  UA000000107204051    Linux  (None, None, None)     mattresses   \n",
       "350777  UA000000107204375  Android  (None, None, None)           main   \n",
       "350778  UA000000107204421  Android  (None, None, None)           main   \n",
       "350779  UA000000107204430  Android  (None, None, None)           main   \n",
       "\n",
       "        event_previous_timestamp   event_timestamp                geo  \\\n",
       "0                   1.593775e+15  1593774992380669        (Indio, CA)   \n",
       "1                   1.593775e+15  1593774944098915        (Indio, CA)   \n",
       "2                   1.593773e+15  1593774779043487        (Indio, CA)   \n",
       "3                   1.593353e+15  1593775861745448      (Atlanta, GA)   \n",
       "4                   1.593776e+15  1593776070318936      (Atlanta, GA)   \n",
       "...                          ...               ...                ...   \n",
       "350775                       NaN  1593813352318609        (Kirby, TX)   \n",
       "350776                       NaN  1593813431096000       (Austin, TX)   \n",
       "350777                       NaN  1593813539955034      (Leawood, KS)   \n",
       "350778                       NaN  1593813553558908    (Corsicana, TX)   \n",
       "350779                       NaN  1593813556338287  (Taylorville, IL)   \n",
       "\n",
       "                                                    items traffic_source  \\\n",
       "0       [(NEWBED10, M_STAN_K, Standard King Mattress, ...          email   \n",
       "1       [(NEWBED10, M_STAN_K, Standard King Mattress, ...          email   \n",
       "2       [(NEWBED10, M_STAN_K, Standard King Mattress, ...          email   \n",
       "3                                                      []          email   \n",
       "4       [(NEWBED10, M_STAN_K, Standard King Mattress, ...          email   \n",
       "...                                                   ...            ...   \n",
       "350775                                                 []         google   \n",
       "350776                                                 []         google   \n",
       "350777                                                 []      instagram   \n",
       "350778                                                 []         google   \n",
       "350779                                                 []         google   \n",
       "\n",
       "        user_first_touch_timestamp  hour                  createdAt  \\\n",
       "0                 1593277246048925    11 2020-07-03 11:16:32.380669   \n",
       "1                 1593277246048925    11 2020-07-03 11:15:44.098915   \n",
       "2                 1593277246048925    11 2020-07-03 11:12:59.043487   \n",
       "3                 1593352019370286    11 2020-07-03 11:31:01.745448   \n",
       "4                 1593352019370286    11 2020-07-03 11:34:30.318936   \n",
       "...                            ...   ...                        ...   \n",
       "350775            1593813352318609    21 2020-07-03 21:55:52.318609   \n",
       "350776            1593813431096000    21 2020-07-03 21:57:11.096000   \n",
       "350777            1593813539955034    21 2020-07-03 21:58:59.955034   \n",
       "350778            1593813553558908    21 2020-07-03 21:59:13.558908   \n",
       "350779            1593813556338287    21 2020-07-03 21:59:16.338287   \n",
       "\n",
       "                                email  \n",
       "0       durhampeter@gomez-houston.com  \n",
       "1       durhampeter@gomez-houston.com  \n",
       "2       durhampeter@gomez-houston.com  \n",
       "3                ericanovak@yahoo.com  \n",
       "4                ericanovak@yahoo.com  \n",
       "...                               ...  \n",
       "350775                           None  \n",
       "350776                           None  \n",
       "350777                           None  \n",
       "350778                           None  \n",
       "350779                           None  \n",
       "\n",
       "[350780 rows x 13 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.format(\"delta\").load(\"spark-warehouse/join_stream/\")\n",
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6ddcecc-b7c5-427e-bbd7-fea0f4886160",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for stream in spark.streams.active:\n",
    "  stream.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2ffb003-abb3-4689-9ad0-8564ac25e1cb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Further reading\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.window.html  \n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.withWatermark.html  \n",
    "https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html  \n",
    "https://docs.databricks.com/spark/latest/structured-streaming/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b127445-2b2f-49fb-84fe-fc9063f2450f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Task 1\n",
    "\n",
    "Create a streaming dataframe from the data in the following path:  \n",
    "`flights200701stream`\n",
    "\n",
    "The schema should contain\n",
    "* DepartureAt (timestamp)\n",
    "* UniqueCarrier (string)\n",
    "\n",
    "Process only 1 file per trigger.  \n",
    "\n",
    "Aggregate the data by count, using non-overlapping 30 minute windows.  \n",
    "Ignore any data that is older than 6 hours.\n",
    "\n",
    "The output should have 3 columns: startTime (window start time), UniqueCarrier, count.  \n",
    "\n",
    "Save to a delta table, firing the trigger every 5 seconds.\n",
    "\n",
    "Display the table, the output should be sorted ascending by startTime.\n",
    "\n",
    "Once the stream has produced some output, call the stream shutdown function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4204d127-eb1f-4c1f-be71-dd4c638935f2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Your answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "Join the Kafka streaming orders dataframe to the `product.csv` dataset.  \n",
    "Note that Spark assumes that any streaming dataframes refer to a directory, not a specific file.\n",
    "\n",
    "Aggregate the data by sum(price) (`total_price`) and a 2-minute tumbling window.  \n",
    "Add a 10 minute watermark, and store the data in a delta table.\n",
    "\n",
    "Create a view on top of the delta table with the following columns:\n",
    "* product_id\n",
    "* product_name\n",
    "* n_minus_2_window_total_price\n",
    "* n_minus_1_window_total_price\n",
    "* current_window_total_price\n",
    "\n",
    "The total_price columns need to be pivoted based on only the 3 most recent windows. \n",
    "The actual \n",
    "Order the dataset by descending `current_total_price`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Practice session - Structured Streaming continued",
   "notebookOrigID": 3621743426785043,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
